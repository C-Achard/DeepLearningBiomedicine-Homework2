

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Graph Convolutional Network &#8212; Deep Learning in Biomedicine - Homework 2</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=c5ced968eda925caa686" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=c5ced968eda925caa686" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=c5ced968eda925caa686" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=c5ced968eda925caa686" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=c5ced968eda925caa686" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=c5ced968eda925caa686" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=c5ced968eda925caa686"></script>

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'rendered_notebooks/GCN';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="GraphSAGE" href="SAGE.html" />
    <link rel="prev" title="Summary of model performance" href="../wandb_comparisons/best_runs.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Deep Learning in Biomedicine - Homework 2 - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Deep Learning in Biomedicine - Homework 2 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Dataset &amp; preprocessing</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Dataset.html">MUTAG Dataset</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hyperparameter tuning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../wandb_comparisons/GCN_comparison.html">Hyperparameter tuning for GCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wandb_comparisons/SAGE_comparison.html">Hyperparameter tuning for GraphSAGE</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wandb_comparisons/AttGCN_comparison.html">Hyperparameter tuning for Attention GCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wandb_comparisons/EdgeAttGCN_comparison.html">Hyperparameter tuning for Edge-Enhanced Attention GCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wandb_comparisons/best_runs.html">Summary of model performance</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Best models notebooks</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Graph Convolutional Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="SAGE.html">GraphSAGE</a></li>
<li class="toctree-l1"><a class="reference internal" href="AttentionGCN.html">Attention GCN</a></li>
<li class="toctree-l1"><a class="reference internal" href="EdgeAttentionGCN.html">Edge-Enhanced Attention GCN</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/C-Achard/DeepLearningBiomedicine-Homework2" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/rendered_notebooks/GCN.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Graph Convolutional Network</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#description">Description</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-loading">Dataset loading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop">Training loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plots-of-training-history">Plots of training history</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-on-unseen-data">Validation on unseen data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#full-dataset-performance">Full dataset performance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-roc-curve">Full ROC curve</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions-distribution">Predictions distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-mislabelled-molecules">Check mislabelled molecules</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="graph-convolutional-network">
<span id="section-gcn"></span><h1>Graph Convolutional Network<a class="headerlink" href="#graph-convolutional-network" title="Permalink to this heading">#</a></h1>
<section id="description">
<h2>Description<a class="headerlink" href="#description" title="Permalink to this heading">#</a></h2>
<p>In this notebook we show an implementation of the Graph Convolutional Network (GCN) for the classification of molecules as mutagenic or non-mutagenic with the MUTAG dataset.</p>
<p>Here we simply define a basic model with several GCN layers of varying shape, and we will find the best hyperparameters.</p>
<p>We will try several variants of :</p>
<ul class="simple">
<li><p>Activation functions</p></li>
<li><p>Pooling layers</p></li>
<li><p>Learning rates</p></li>
<li><p>Normalization layers</p></li>
<li><p>Weighting the loss to compensate for class imbalance</p></li>
</ul>
</section>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this heading">#</a></h2>
<section id="dataset-loading">
<h3>Dataset loading<a class="headerlink" href="#dataset-loading" title="Permalink to this heading">#</a></h3>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sys</span> <span class="kn">import</span> <span class="n">path</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">environ</span>

<span class="n">environ</span><span class="p">[</span><span class="s2">&quot;WANDB_NOTEBOOK_NAME&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;GCN.ipynb&quot;</span>  <span class="c1"># set notebook name for wandb</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span>
    <span class="s2">&quot;ignore&quot;</span>
<span class="p">)</span>  <span class="c1"># ignore warnings from missing deterministic implementation (from wandb, not model itself)</span>
<span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;../code&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">model</span> <span class="k">as</span> <span class="nn">m</span>
<span class="kn">import</span> <span class="nn">training</span> <span class="k">as</span> <span class="nn">t</span>
<span class="kn">import</span> <span class="nn">utils</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">LOG</span> <span class="k">as</span> <span class="n">logger</span>

<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">%(message)s</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span>
<span class="p">)</span>
<span class="n">logger</span><span class="o">.</span><span class="n">setLevel</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>
<span class="c1"># logger.setLevel(logging.DEBUG)</span>

<span class="n">t</span><span class="o">.</span><span class="n">WANDB_MODE</span> <span class="o">=</span> <span class="s2">&quot;disabled&quot;</span>

<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataloaders</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">create_dataloaders</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Length of train set: 131
Length of validation set: 28
Length of test set: 29
</pre></div>
</div>
</div>
</div>
</section>
<section id="model">
<h3>Model<a class="headerlink" href="#model" title="Permalink to this heading">#</a></h3>
<p>Let’s initialize the model with our chosen architecture !</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">node_features</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">node_features</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">conv_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">GCN</span><span class="p">(</span>
    <span class="n">num_features</span><span class="o">=</span><span class="n">node_features</span><span class="p">,</span>
    <span class="n">conv_dims</span><span class="o">=</span><span class="n">conv_dims</span><span class="p">,</span>
    <span class="c1"># fcn_layers=[128],</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">(),</span>
    <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">pooling</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span>
    <span class="n">norm</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># utils._print_gradient_hook(model)</span>


<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initialized model with 4 graph conv layers
Initialized model with 1 fully connected layers
GCN(
  (convs_layers): ModuleList(
    (0): GraphConv(
      (weight): Linear(in_features=7, out_features=256, bias=False)
      (bias): Linear(in_features=7, out_features=256, bias=False)
      (activation): SELU()
    )
    (1): GraphConv(
      (weight): Linear(in_features=256, out_features=256, bias=False)
      (bias): Linear(in_features=256, out_features=256, bias=False)
      (activation): SELU()
    )
    (2): GraphConv(
      (weight): Linear(in_features=256, out_features=128, bias=False)
      (bias): Linear(in_features=256, out_features=128, bias=False)
      (activation): SELU()
    )
    (3): GraphConv(
      (weight): Linear(in_features=128, out_features=64, bias=False)
      (bias): Linear(in_features=128, out_features=64, bias=False)
      (activation): Identity()
    )
  )
  (batch_norms): ModuleList(
    (0-1): 2 x BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (3): Identity()
  )
  (fcn_layers): ModuleList(
    (0): Linear(in_features=64, out_features=1, bias=True)
  )
  (dropout): Dropout(p=0.2, inplace=False)
  (pooling): MaxPooling()
)
</pre></div>
</div>
</div>
</div>
</section>
<section id="parameters">
<h3>Parameters<a class="headerlink" href="#parameters" title="Permalink to this heading">#</a></h3>
<p>Now we create the loss function and the optimizer.
We will use the Binary Cross Entropy with the built-in sigmoid for stability,
and use Adam as the optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="n">label_counts</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">class_y</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">pos_weight</span> <span class="o">=</span> <span class="n">label_counts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">label_counts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">loss_fn</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCEWithLogitsLoss</span><span class="p">(</span>
    <span class="c1"># pos_weight=torch.tensor(pos_weight) # NOTE : due to an issue with reproducibility, this is not used here, in order to obtain the selected best run. </span>
                                          <span class="c1"># It is however used for the other models.</span>
<span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;epoch&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;val-roc&quot;</span><span class="p">:</span> <span class="p">[],</span>
    <span class="s2">&quot;val-ap&quot;</span><span class="p">:</span> <span class="p">[],</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="training-loop">
<h3>Training loop<a class="headerlink" href="#training-loop" title="Permalink to this heading">#</a></h3>
<p>Here we train the model.</p>
<div class="cell tag_output_scroll docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">train_loop</span><span class="p">(</span>
    <span class="n">history</span><span class="o">=</span><span class="n">history</span><span class="p">,</span>
    <span class="n">train_dataloader</span><span class="o">=</span><span class="n">dataloaders</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">val_dataloader</span><span class="o">=</span><span class="n">dataloaders</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
    <span class="n">loss_fn</span><span class="o">=</span><span class="n">loss_fn</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">,</span>
    <span class="n">use_scheduler</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">test_dataloader</span><span class="o">=</span><span class="n">dataloaders</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Watching
Epoch   1/1000:Epoch loss: 0.7356 - avg acc: 30.5% - val-roc: 0.7041 - val-ap: 0.6378 (0.7s/epoch)
Epoch   2/1000:Epoch loss: 0.6209 - avg acc: 32.1% - val-roc: 0.8622 - val-ap: 0.9041 (0.1s/epoch)
Epoch   3/1000:Epoch loss: 0.5704 - avg acc: 33.6% - val-roc: 0.8265 - val-ap: 0.8750 (0.1s/epoch)
Epoch   4/1000:Epoch loss: 0.5038 - avg acc: 32.8% - val-roc: 0.7908 - val-ap: 0.8544 (0.1s/epoch)
Epoch   5/1000:Epoch loss: 0.5551 - avg acc: 51.1% - val-roc: 0.7755 - val-ap: 0.8435 (0.1s/epoch)
Epoch   6/1000:Epoch loss: 0.5733 - avg acc: 32.8% - val-roc: 0.7194 - val-ap: 0.7249 (0.1s/epoch)
Epoch   7/1000:Epoch loss: 0.5114 - avg acc: 32.8% - val-roc: 0.7245 - val-ap: 0.6958 (0.1s/epoch)
Epoch   8/1000:Epoch loss: 0.4970 - avg acc: 31.3% - val-roc: 0.7347 - val-ap: 0.7392 (0.1s/epoch)
Epoch   9/1000:Epoch loss: 0.4883 - avg acc: 31.3% - val-roc: 0.6173 - val-ap: 0.5557 (0.1s/epoch)
Epoch  10/1000:Epoch loss: 0.5239 - avg acc: 32.8% - val-roc: 0.6122 - val-ap: 0.5539 (0.2s/epoch)
Epoch  11/1000:Epoch loss: 0.4593 - avg acc: 31.3% - val-roc: 0.6020 - val-ap: 0.5393 (0.1s/epoch)
Epoch  12/1000:Epoch loss: 0.4926 - avg acc: 35.9% - val-roc: 0.6020 - val-ap: 0.5440 (0.1s/epoch)
Epoch  13/1000:Epoch loss: 0.5245 - avg acc: 31.3% - val-roc: 0.5969 - val-ap: 0.5410 (0.1s/epoch)
Epoch  14/1000:Epoch loss: 0.4493 - avg acc: 35.9% - val-roc: 0.6786 - val-ap: 0.6279 (0.1s/epoch)
Epoch  15/1000:Epoch loss: 0.4713 - avg acc: 32.1% - val-roc: 0.6582 - val-ap: 0.6108 (0.1s/epoch)
Epoch  16/1000:Epoch loss: 0.5769 - avg acc: 32.1% - val-roc: 0.6224 - val-ap: 0.5731 (0.1s/epoch)
Epoch  17/1000:Epoch loss: 0.4769 - avg acc: 32.1% - val-roc: 0.5918 - val-ap: 0.5403 (0.1s/epoch)
Epoch  18/1000:Epoch loss: 0.5488 - avg acc: 33.6% - val-roc: 0.5867 - val-ap: 0.5387 (0.1s/epoch)
Epoch  19/1000:Epoch loss: 0.5141 - avg acc: 34.4% - val-roc: 0.5663 - val-ap: 0.5260 (0.1s/epoch)
Epoch  20/1000:Epoch loss: 0.4777 - avg acc: 32.1% - val-roc: 0.5969 - val-ap: 0.5395 (0.1s/epoch)
Epoch  21/1000:Epoch loss: 0.4603 - avg acc: 32.1% - val-roc: 0.6633 - val-ap: 0.6126 (0.1s/epoch)
Epoch  22/1000:Epoch loss: 0.5492 - avg acc: 32.1% - val-roc: 0.6480 - val-ap: 0.5947 (0.1s/epoch)
Epoch  23/1000:Epoch loss: 0.4805 - avg acc: 32.1% - val-roc: 0.6735 - val-ap: 0.6584 (0.1s/epoch)
Epoch  24/1000:Epoch loss: 0.4581 - avg acc: 32.1% - val-roc: 0.6684 - val-ap: 0.6511 (0.1s/epoch)
Epoch  25/1000:Epoch loss: 0.4338 - avg acc: 32.1% - val-roc: 0.6122 - val-ap: 0.5571 (0.1s/epoch)
Epoch  26/1000:Epoch loss: 0.4508 - avg acc: 32.1% - val-roc: 0.5765 - val-ap: 0.5230 (0.1s/epoch)
Epoch  27/1000:Epoch loss: 0.4543 - avg acc: 32.8% - val-roc: 0.5663 - val-ap: 0.5166 (0.1s/epoch)
Epoch  28/1000:Epoch loss: 0.4818 - avg acc: 32.8% - val-roc: 0.5816 - val-ap: 0.5301 (0.1s/epoch)
Epoch  29/1000:Epoch loss: 0.4509 - avg acc: 32.1% - val-roc: 0.5561 - val-ap: 0.5092 (0.1s/epoch)
Epoch  30/1000:Epoch loss: 0.4342 - avg acc: 32.1% - val-roc: 0.5357 - val-ap: 0.4924 (0.1s/epoch)
Epoch  31/1000:Epoch loss: 0.4419 - avg acc: 32.1% - val-roc: 0.5459 - val-ap: 0.5048 (0.1s/epoch)
Epoch  32/1000:Epoch loss: 0.4680 - avg acc: 32.1% - val-roc: 0.5357 - val-ap: 0.5002 (0.1s/epoch)
Epoch  33/1000:Epoch loss: 0.4572 - avg acc: 32.1% - val-roc: 0.5459 - val-ap: 0.5067 (0.1s/epoch)
Epoch  34/1000:Epoch loss: 0.5066 - avg acc: 34.4% - val-roc: 0.5816 - val-ap: 0.5333 (0.1s/epoch)
Epoch  35/1000:Epoch loss: 0.4705 - avg acc: 32.1% - val-roc: 0.5867 - val-ap: 0.5370 (0.1s/epoch)
Epoch  36/1000:Epoch loss: 0.4131 - avg acc: 32.8% - val-roc: 0.5918 - val-ap: 0.5388 (0.2s/epoch)
Epoch  37/1000:Epoch loss: 0.4507 - avg acc: 32.8% - val-roc: 0.5918 - val-ap: 0.5388 (0.1s/epoch)
Epoch  38/1000:Epoch loss: 0.4160 - avg acc: 33.6% - val-roc: 0.5969 - val-ap: 0.5355 (0.1s/epoch)
Epoch  39/1000:Epoch loss: 0.5063 - avg acc: 33.6% - val-roc: 0.5918 - val-ap: 0.5370 (0.1s/epoch)
Epoch  40/1000:Epoch loss: 0.4590 - avg acc: 32.1% - val-roc: 0.5714 - val-ap: 0.5252 (0.1s/epoch)
Epoch  41/1000:Epoch loss: 0.4190 - avg acc: 33.6% - val-roc: 0.5867 - val-ap: 0.5301 (0.1s/epoch)
Epoch  42/1000:Epoch loss: 0.4441 - avg acc: 32.1% - val-roc: 0.5561 - val-ap: 0.5064 (0.1s/epoch)
Epoch  43/1000:Epoch loss: 0.4563 - avg acc: 32.8% - val-roc: 0.5969 - val-ap: 0.5372 (0.1s/epoch)
Epoch  44/1000:Epoch loss: 0.4446 - avg acc: 33.6% - val-roc: 0.6071 - val-ap: 0.5446 (0.1s/epoch)
Epoch  45/1000:Epoch loss: 0.4323 - avg acc: 35.1% - val-roc: 0.6071 - val-ap: 0.5427 (0.1s/epoch)
Epoch  46/1000:Epoch loss: 0.4420 - avg acc: 33.6% - val-roc: 0.5969 - val-ap: 0.5377 (0.1s/epoch)
Epoch  47/1000:Epoch loss: 0.4402 - avg acc: 33.6% - val-roc: 0.5969 - val-ap: 0.5387 (0.1s/epoch)
Epoch  48/1000:Epoch loss: 0.4742 - avg acc: 33.6% - val-roc: 0.5918 - val-ap: 0.5350 (0.1s/epoch)
Epoch  49/1000:Epoch loss: 0.4813 - avg acc: 33.6% - val-roc: 0.5816 - val-ap: 0.5282 (0.1s/epoch)
Epoch  50/1000:Epoch loss: 0.3936 - avg acc: 34.4% - val-roc: 0.5765 - val-ap: 0.5251 (0.1s/epoch)
Epoch  51/1000:Epoch loss: 0.4181 - avg acc: 32.8% - val-roc: 0.5765 - val-ap: 0.5262 (0.1s/epoch)
Epoch  52/1000:Epoch loss: 0.4241 - avg acc: 32.1% - val-roc: 0.5510 - val-ap: 0.5074 (0.1s/epoch)
Epoch  53/1000:Epoch loss: 0.4043 - avg acc: 35.1% - val-roc: 0.5663 - val-ap: 0.5202 (0.1s/epoch)
Epoch  54/1000:Epoch loss: 0.4405 - avg acc: 33.6% - val-roc: 0.5459 - val-ap: 0.5042 (0.1s/epoch)
Epoch  55/1000:Epoch loss: 0.4304 - avg acc: 35.1% - val-roc: 0.5714 - val-ap: 0.5219 (0.1s/epoch)
Epoch  56/1000:Epoch loss: 0.4289 - avg acc: 35.9% - val-roc: 0.5510 - val-ap: 0.5078 (0.1s/epoch)
Epoch  57/1000:Epoch loss: 0.4475 - avg acc: 33.6% - val-roc: 0.5459 - val-ap: 0.5005 (0.1s/epoch)
Epoch  58/1000:Epoch loss: 0.4136 - avg acc: 35.9% - val-roc: 0.5816 - val-ap: 0.5258 (0.1s/epoch)
Epoch  59/1000:Epoch loss: 0.4150 - avg acc: 34.4% - val-roc: 0.5816 - val-ap: 0.5262 (0.1s/epoch)
Epoch  60/1000:Epoch loss: 0.4680 - avg acc: 35.1% - val-roc: 0.5306 - val-ap: 0.4933 (0.1s/epoch)
Epoch  61/1000:Epoch loss: 0.4149 - avg acc: 33.6% - val-roc: 0.5204 - val-ap: 0.4876 (0.1s/epoch)
Epoch  62/1000:Epoch loss: 0.4185 - avg acc: 36.6% - val-roc: 0.5204 - val-ap: 0.4871 (0.1s/epoch)
Epoch  63/1000:Epoch loss: 0.4150 - avg acc: 34.4% - val-roc: 0.5153 - val-ap: 0.4856 (0.1s/epoch)
Epoch  64/1000:Epoch loss: 0.5277 - avg acc: 35.9% - val-roc: 0.5153 - val-ap: 0.4856 (0.1s/epoch)
Epoch  65/1000:Epoch loss: 0.4291 - avg acc: 35.1% - val-roc: 0.5204 - val-ap: 0.4876 (0.1s/epoch)
Epoch  66/1000:Epoch loss: 0.3790 - avg acc: 35.9% - val-roc: 0.5204 - val-ap: 0.4876 (0.2s/epoch)
Epoch  67/1000:Epoch loss: 0.4086 - avg acc: 35.1% - val-roc: 0.5204 - val-ap: 0.4843 (0.1s/epoch)
Epoch  68/1000:Epoch loss: 0.4300 - avg acc: 35.1% - val-roc: 0.5204 - val-ap: 0.4876 (0.4s/epoch)
Epoch  69/1000:Epoch loss: 0.4335 - avg acc: 35.1% - val-roc: 0.5102 - val-ap: 0.4805 (0.2s/epoch)
Epoch  70/1000:Epoch loss: 0.3881 - avg acc: 35.1% - val-roc: 0.5102 - val-ap: 0.4836 (0.3s/epoch)
Epoch  71/1000:Epoch loss: 0.4211 - avg acc: 35.1% - val-roc: 0.5102 - val-ap: 0.4836 (0.1s/epoch)
Epoch  72/1000:Epoch loss: 0.4158 - avg acc: 35.1% - val-roc: 0.5000 - val-ap: 0.4758 (0.1s/epoch)
Epoch  73/1000:Epoch loss: 0.4101 - avg acc: 35.1% - val-roc: 0.5102 - val-ap: 0.4833 (0.1s/epoch)
Epoch  74/1000:Epoch loss: 0.3905 - avg acc: 35.1% - val-roc: 0.5204 - val-ap: 0.4843 (0.1s/epoch)
Epoch  75/1000:Epoch loss: 0.3914 - avg acc: 35.9% - val-roc: 0.5204 - val-ap: 0.4867 (0.1s/epoch)
Epoch  76/1000:Epoch loss: 0.4357 - avg acc: 35.9% - val-roc: 0.5204 - val-ap: 0.4843 (0.2s/epoch)
Epoch  77/1000:Epoch loss: 0.4343 - avg acc: 35.1% - val-roc: 0.5153 - val-ap: 0.4847 (0.1s/epoch)
Epoch  78/1000:Epoch loss: 0.5099 - avg acc: 35.9% - val-roc: 0.5153 - val-ap: 0.4847 (0.1s/epoch)
Epoch  79/1000:Epoch loss: 0.5545 - avg acc: 35.9% - val-roc: 0.5306 - val-ap: 0.4919 (0.1s/epoch)
Epoch  80/1000:Epoch loss: 0.4086 - avg acc: 34.4% - val-roc: 0.5816 - val-ap: 0.5258 (0.1s/epoch)
Epoch  81/1000:Epoch loss: 0.4394 - avg acc: 35.9% - val-roc: 0.6582 - val-ap: 0.5933 (0.1s/epoch)
Epoch  82/1000:Epoch loss: 0.4079 - avg acc: 33.6% - val-roc: 0.6480 - val-ap: 0.6009 (0.1s/epoch)
Epoch  83/1000:Epoch loss: 0.4160 - avg acc: 37.4% - val-roc: 0.6633 - val-ap: 0.6073 (0.1s/epoch)
Epoch  84/1000:Epoch loss: 0.4539 - avg acc: 34.4% - val-roc: 0.6378 - val-ap: 0.5804 (0.1s/epoch)
Epoch  85/1000:Epoch loss: 0.4056 - avg acc: 35.1% - val-roc: 0.6276 - val-ap: 0.5636 (0.1s/epoch)
Epoch  86/1000:Epoch loss: 0.4438 - avg acc: 34.4% - val-roc: 0.6173 - val-ap: 0.5583 (0.1s/epoch)
Epoch  87/1000:Epoch loss: 0.3839 - avg acc: 35.1% - val-roc: 0.5918 - val-ap: 0.5395 (0.1s/epoch)
Epoch  88/1000:Epoch loss: 0.3994 - avg acc: 35.1% - val-roc: 0.5102 - val-ap: 0.4824 (0.1s/epoch)
Epoch  89/1000:Epoch loss: 0.4346 - avg acc: 36.6% - val-roc: 0.5255 - val-ap: 0.4920 (0.1s/epoch)
Epoch  90/1000:Epoch loss: 0.4373 - avg acc: 35.9% - val-roc: 0.6020 - val-ap: 0.5436 (0.1s/epoch)
Epoch  91/1000:Epoch loss: 0.4715 - avg acc: 35.9% - val-roc: 0.6327 - val-ap: 0.5638 (0.1s/epoch)
Epoch  92/1000:Epoch loss: 0.4594 - avg acc: 35.1% - val-roc: 0.6327 - val-ap: 0.5648 (0.1s/epoch)
Epoch  93/1000:Epoch loss: 0.4508 - avg acc: 34.4% - val-roc: 0.6224 - val-ap: 0.5582 (0.1s/epoch)
Epoch  94/1000:Epoch loss: 0.3507 - avg acc: 36.6% - val-roc: 0.5969 - val-ap: 0.5397 (0.2s/epoch)
Epoch  95/1000:Epoch loss: 0.4254 - avg acc: 33.6% - val-roc: 0.5306 - val-ap: 0.4908 (0.1s/epoch)
Epoch  96/1000:Epoch loss: 0.4024 - avg acc: 35.9% - val-roc: 0.5714 - val-ap: 0.5179 (0.1s/epoch)
Epoch  97/1000:Epoch loss: 0.3747 - avg acc: 34.4% - val-roc: 0.5051 - val-ap: 0.4779 (0.1s/epoch)
Epoch  98/1000:Epoch loss: 0.4240 - avg acc: 37.4% - val-roc: 0.5510 - val-ap: 0.5084 (0.2s/epoch)
Epoch  99/1000:Epoch loss: 0.4507 - avg acc: 35.1% - val-roc: 0.5510 - val-ap: 0.5084 (0.1s/epoch)
Epoch  100/1000:Epoch loss: 0.3873 - avg acc: 58.0% - val-roc: 0.6276 - val-ap: 0.5649 (0.1s/epoch)
Epoch  101/1000:Epoch loss: 0.4196 - avg acc: 34.4% - val-roc: 0.5255 - val-ap: 0.4916 (0.1s/epoch)
Epoch  102/1000:Epoch loss: 0.4220 - avg acc: 37.4% - val-roc: 0.6173 - val-ap: 0.5587 (0.1s/epoch)
Epoch  103/1000:Epoch loss: 0.4253 - avg acc: 35.1% - val-roc: 0.6173 - val-ap: 0.5609 (0.1s/epoch)
Epoch  104/1000:Epoch loss: 0.3950 - avg acc: 35.1% - val-roc: 0.6122 - val-ap: 0.5618 (0.1s/epoch)
Epoch  105/1000:Epoch loss: 0.3771 - avg acc: 35.1% - val-roc: 0.6122 - val-ap: 0.5596 (0.1s/epoch)
Epoch  106/1000:Epoch loss: 0.4158 - avg acc: 35.1% - val-roc: 0.6071 - val-ap: 0.5403 (0.1s/epoch)
Epoch  107/1000:Epoch loss: 0.4003 - avg acc: 35.9% - val-roc: 0.6020 - val-ap: 0.5380 (0.1s/epoch)
Epoch  108/1000:Epoch loss: 0.3499 - avg acc: 35.1% - val-roc: 0.5969 - val-ap: 0.5363 (0.1s/epoch)
Epoch  109/1000:Epoch loss: 0.3699 - avg acc: 40.5% - val-roc: 0.6327 - val-ap: 0.5692 (0.1s/epoch)
Epoch  110/1000:Epoch loss: 0.3636 - avg acc: 35.1% - val-roc: 0.5969 - val-ap: 0.5442 (0.1s/epoch)
Epoch  111/1000:Epoch loss: 0.4005 - avg acc: 38.2% - val-roc: 0.5765 - val-ap: 0.5276 (0.1s/epoch)
Epoch  112/1000:Epoch loss: 0.3716 - avg acc: 38.9% - val-roc: 0.5459 - val-ap: 0.5085 (0.1s/epoch)
Epoch  113/1000:Epoch loss: 0.4202 - avg acc: 38.9% - val-roc: 0.5357 - val-ap: 0.5016 (0.1s/epoch)
Epoch  114/1000:Epoch loss: 0.3606 - avg acc: 38.9% - val-roc: 0.5204 - val-ap: 0.4945 (0.1s/epoch)
Epoch  115/1000:Epoch loss: 0.3709 - avg acc: 35.9% - val-roc: 0.5102 - val-ap: 0.4871 (0.1s/epoch)
Epoch  116/1000:Epoch loss: 0.3882 - avg acc: 38.2% - val-roc: 0.5561 - val-ap: 0.5213 (0.1s/epoch)
Epoch  117/1000:Epoch loss: 0.3875 - avg acc: 36.6% - val-roc: 0.6378 - val-ap: 0.5926 (0.1s/epoch)
Epoch  118/1000:Epoch loss: 0.3758 - avg acc: 36.6% - val-roc: 0.6378 - val-ap: 0.5948 (0.1s/epoch)
Epoch  119/1000:Epoch loss: 0.3911 - avg acc: 35.9% - val-roc: 0.5561 - val-ap: 0.5076 (0.1s/epoch)
Epoch  120/1000:Epoch loss: 0.3906 - avg acc: 35.9% - val-roc: 0.5459 - val-ap: 0.5041 (0.1s/epoch)
Epoch  121/1000:Epoch loss: 0.3794 - avg acc: 39.7% - val-roc: 0.5408 - val-ap: 0.5006 (0.1s/epoch)
Epoch  122/1000:Epoch loss: 0.3738 - avg acc: 35.1% - val-roc: 0.5357 - val-ap: 0.4993 (0.1s/epoch)
Epoch  123/1000:Epoch loss: 0.4236 - avg acc: 38.2% - val-roc: 0.5612 - val-ap: 0.5128 (0.1s/epoch)
Epoch  124/1000:Epoch loss: 0.4006 - avg acc: 35.9% - val-roc: 0.5561 - val-ap: 0.5139 (0.1s/epoch)
Epoch  125/1000:Epoch loss: 0.4464 - avg acc: 39.7% - val-roc: 0.5765 - val-ap: 0.5276 (0.1s/epoch)
Epoch  126/1000:Epoch loss: 0.3990 - avg acc: 34.4% - val-roc: 0.5816 - val-ap: 0.5311 (0.1s/epoch)
Epoch  127/1000:Epoch loss: 0.3983 - avg acc: 39.7% - val-roc: 0.5918 - val-ap: 0.5361 (0.1s/epoch)
Epoch  128/1000:Epoch loss: 0.3749 - avg acc: 37.4% - val-roc: 0.6122 - val-ap: 0.5514 (0.1s/epoch)
Epoch  129/1000:Epoch loss: 0.3866 - avg acc: 36.6% - val-roc: 0.5765 - val-ap: 0.5304 (0.1s/epoch)
Epoch  130/1000:Epoch loss: 0.4007 - avg acc: 39.7% - val-roc: 0.5408 - val-ap: 0.5099 (0.1s/epoch)
Epoch  131/1000:Epoch loss: 0.3650 - avg acc: 36.6% - val-roc: 0.5255 - val-ap: 0.4964 (0.1s/epoch)
Epoch  132/1000:Epoch loss: 0.3759 - avg acc: 37.4% - val-roc: 0.5612 - val-ap: 0.5168 (0.1s/epoch)
Epoch  133/1000:Epoch loss: 0.3694 - avg acc: 40.5% - val-roc: 0.5561 - val-ap: 0.5146 (0.1s/epoch)
Epoch  134/1000:Epoch loss: 0.3571 - avg acc: 39.7% - val-roc: 0.5561 - val-ap: 0.5113 (0.1s/epoch)
Epoch  135/1000:Epoch loss: 0.4256 - avg acc: 37.4% - val-roc: 0.5969 - val-ap: 0.5347 (0.1s/epoch)
Epoch  136/1000:Epoch loss: 0.3580 - avg acc: 38.9% - val-roc: 0.6020 - val-ap: 0.5390 (0.1s/epoch)
Epoch  137/1000:Epoch loss: 0.3653 - avg acc: 40.5% - val-roc: 0.5918 - val-ap: 0.5405 (0.1s/epoch)
Epoch  138/1000:Epoch loss: 0.3973 - avg acc: 36.6% - val-roc: 0.5663 - val-ap: 0.5176 (0.1s/epoch)
Epoch  139/1000:Epoch loss: 0.3670 - avg acc: 38.9% - val-roc: 0.5612 - val-ap: 0.5143 (0.1s/epoch)
Epoch  140/1000:Epoch loss: 0.3255 - avg acc: 37.4% - val-roc: 0.5408 - val-ap: 0.5044 (0.1s/epoch)
Epoch  141/1000:Epoch loss: 0.3705 - avg acc: 38.9% - val-roc: 0.5408 - val-ap: 0.4994 (0.1s/epoch)
Epoch  142/1000:Epoch loss: 0.4282 - avg acc: 39.7% - val-roc: 0.5714 - val-ap: 0.5205 (0.1s/epoch)
Epoch  143/1000:Epoch loss: 0.3661 - avg acc: 36.6% - val-roc: 0.5765 - val-ap: 0.5220 (0.1s/epoch)
Epoch  144/1000:Epoch loss: 0.3984 - avg acc: 36.6% - val-roc: 0.5510 - val-ap: 0.5132 (0.1s/epoch)
Epoch  145/1000:Epoch loss: 0.3734 - avg acc: 37.4% - val-roc: 0.5561 - val-ap: 0.5109 (0.1s/epoch)
Epoch  146/1000:Epoch loss: 0.3716 - avg acc: 39.7% - val-roc: 0.5408 - val-ap: 0.5039 (0.1s/epoch)
Epoch  147/1000:Epoch loss: 0.3641 - avg acc: 35.1% - val-roc: 0.5459 - val-ap: 0.5055 (0.1s/epoch)
Epoch  148/1000:Epoch loss: 0.3703 - avg acc: 35.9% - val-roc: 0.5561 - val-ap: 0.5115 (0.1s/epoch)
Epoch  149/1000:Epoch loss: 0.3511 - avg acc: 35.9% - val-roc: 0.5561 - val-ap: 0.5113 (0.1s/epoch)
Epoch  150/1000:Epoch loss: 0.3343 - avg acc: 38.9% - val-roc: 0.5765 - val-ap: 0.5298 (0.1s/epoch)
Epoch  151/1000:Epoch loss: 0.3677 - avg acc: 35.9% - val-roc: 0.5816 - val-ap: 0.5332 (0.1s/epoch)
Epoch  152/1000:Epoch loss: 0.3494 - avg acc: 35.9% - val-roc: 0.6173 - val-ap: 0.5638 (0.1s/epoch)
Epoch  153/1000:Epoch loss: 0.3763 - avg acc: 55.7% - val-roc: 0.6071 - val-ap: 0.5593 (0.1s/epoch)
Epoch  154/1000:Epoch loss: 0.4055 - avg acc: 39.7% - val-roc: 0.6020 - val-ap: 0.5507 (0.1s/epoch)
Epoch  155/1000:Epoch loss: 0.3791 - avg acc: 39.7% - val-roc: 0.5663 - val-ap: 0.5243 (0.1s/epoch)
Epoch  156/1000:Epoch loss: 0.3621 - avg acc: 39.7% - val-roc: 0.5204 - val-ap: 0.4967 (0.1s/epoch)
Epoch  157/1000:Epoch loss: 0.3379 - avg acc: 39.7% - val-roc: 0.5255 - val-ap: 0.4984 (0.1s/epoch)
Epoch  158/1000:Epoch loss: 0.3602 - avg acc: 40.5% - val-roc: 0.5612 - val-ap: 0.5168 (0.2s/epoch)
Epoch  159/1000:Epoch loss: 0.3240 - avg acc: 41.2% - val-roc: 0.5306 - val-ap: 0.5009 (0.1s/epoch)
Epoch  160/1000:Epoch loss: 0.3260 - avg acc: 40.5% - val-roc: 0.5510 - val-ap: 0.5100 (0.1s/epoch)
Epoch  161/1000:Epoch loss: 0.3772 - avg acc: 39.7% - val-roc: 0.5612 - val-ap: 0.5155 (0.1s/epoch)
Epoch  162/1000:Epoch loss: 0.3520 - avg acc: 40.5% - val-roc: 0.5612 - val-ap: 0.5161 (0.1s/epoch)
Epoch  163/1000:Epoch loss: 0.3409 - avg acc: 41.2% - val-roc: 0.5357 - val-ap: 0.5066 (0.1s/epoch)
Epoch  164/1000:Epoch loss: 0.4023 - avg acc: 36.6% - val-roc: 0.5459 - val-ap: 0.5207 (0.1s/epoch)
Epoch  165/1000:Epoch loss: 0.4035 - avg acc: 61.1% - val-roc: 0.6224 - val-ap: 0.5906 (0.1s/epoch)
Epoch  166/1000:Epoch loss: 0.3114 - avg acc: 41.2% - val-roc: 0.6531 - val-ap: 0.6251 (0.1s/epoch)
Epoch  167/1000:Epoch loss: 0.3231 - avg acc: 41.2% - val-roc: 0.6735 - val-ap: 0.6532 (0.1s/epoch)
Epoch  168/1000:Epoch loss: 0.3611 - avg acc: 38.9% - val-roc: 0.6684 - val-ap: 0.6532 (0.1s/epoch)
Epoch  169/1000:Epoch loss: 0.3970 - avg acc: 46.6% - val-roc: 0.6276 - val-ap: 0.6037 (0.1s/epoch)
Epoch  170/1000:Epoch loss: 0.3550 - avg acc: 41.2% - val-roc: 0.5612 - val-ap: 0.5590 (0.1s/epoch)
Epoch  171/1000:Epoch loss: 0.3617 - avg acc: 39.7% - val-roc: 0.5357 - val-ap: 0.5376 (0.1s/epoch)
Epoch  172/1000:Epoch loss: 0.3232 - avg acc: 57.3% - val-roc: 0.5663 - val-ap: 0.5566 (0.1s/epoch)
Epoch  173/1000:Epoch loss: 0.4046 - avg acc: 39.7% - val-roc: 0.5867 - val-ap: 0.5716 (0.1s/epoch)
Epoch  174/1000:Epoch loss: 0.3509 - avg acc: 57.3% - val-roc: 0.5765 - val-ap: 0.5414 (0.1s/epoch)
Epoch  175/1000:Epoch loss: 0.3550 - avg acc: 58.0% - val-roc: 0.5510 - val-ap: 0.5179 (0.1s/epoch)
Epoch  176/1000:Epoch loss: 0.3521 - avg acc: 41.2% - val-roc: 0.5255 - val-ap: 0.4972 (0.1s/epoch)
Epoch  177/1000:Epoch loss: 0.3414 - avg acc: 42.7% - val-roc: 0.4949 - val-ap: 0.4914 (0.2s/epoch)
Epoch  178/1000:Epoch loss: 0.3667 - avg acc: 43.5% - val-roc: 0.5051 - val-ap: 0.4947 (0.1s/epoch)
Epoch  179/1000:Epoch loss: 0.3730 - avg acc: 42.0% - val-roc: 0.5510 - val-ap: 0.5117 (0.1s/epoch)
Epoch  180/1000:Epoch loss: 0.3581 - avg acc: 61.1% - val-roc: 0.5765 - val-ap: 0.5314 (0.1s/epoch)
Epoch  181/1000:Epoch loss: 0.3296 - avg acc: 40.5% - val-roc: 0.5663 - val-ap: 0.5238 (0.1s/epoch)
Epoch  182/1000:Epoch loss: 0.3580 - avg acc: 62.6% - val-roc: 0.5969 - val-ap: 0.5501 (0.1s/epoch)
Epoch  183/1000:Epoch loss: 0.3437 - avg acc: 42.0% - val-roc: 0.5969 - val-ap: 0.5483 (0.1s/epoch)
Epoch  184/1000:Epoch loss: 0.3672 - avg acc: 42.0% - val-roc: 0.5816 - val-ap: 0.5431 (0.1s/epoch)
Epoch  185/1000:Epoch loss: 0.3663 - avg acc: 61.1% - val-roc: 0.6173 - val-ap: 0.5627 (0.1s/epoch)
Epoch  186/1000:Epoch loss: 0.3081 - avg acc: 51.9% - val-roc: 0.6429 - val-ap: 0.6012 (0.1s/epoch)
Epoch  187/1000:Epoch loss: 0.3182 - avg acc: 62.6% - val-roc: 0.6429 - val-ap: 0.6021 (0.1s/epoch)
Epoch  188/1000:Epoch loss: 0.3435 - avg acc: 62.6% - val-roc: 0.6429 - val-ap: 0.6194 (0.1s/epoch)
Epoch  189/1000:Epoch loss: 0.3083 - avg acc: 51.1% - val-roc: 0.6276 - val-ap: 0.5975 (0.1s/epoch)
Epoch  190/1000:Epoch loss: 0.3653 - avg acc: 50.4% - val-roc: 0.6480 - val-ap: 0.6074 (0.1s/epoch)
Epoch  191/1000:Epoch loss: 0.3636 - avg acc: 46.6% - val-roc: 0.6480 - val-ap: 0.6089 (0.1s/epoch)
Epoch  192/1000:Epoch loss: 0.3579 - avg acc: 68.7% - val-roc: 0.6327 - val-ap: 0.5995 (0.1s/epoch)
Epoch  193/1000:Epoch loss: 0.3545 - avg acc: 45.0% - val-roc: 0.6378 - val-ap: 0.5778 (0.1s/epoch)
Epoch  194/1000:Epoch loss: 0.3164 - avg acc: 64.1% - val-roc: 0.6327 - val-ap: 0.5740 (0.1s/epoch)
Epoch  195/1000:Epoch loss: 0.3435 - avg acc: 42.7% - val-roc: 0.6224 - val-ap: 0.5740 (0.1s/epoch)
Epoch  196/1000:Epoch loss: 0.3263 - avg acc: 45.0% - val-roc: 0.5969 - val-ap: 0.5608 (0.1s/epoch)
Epoch  197/1000:Epoch loss: 0.3286 - avg acc: 43.5% - val-roc: 0.6224 - val-ap: 0.5837 (0.1s/epoch)
Epoch  198/1000:Epoch loss: 0.3564 - avg acc: 47.3% - val-roc: 0.6276 - val-ap: 0.5885 (0.1s/epoch)
Epoch  199/1000:Epoch loss: 0.3460 - avg acc: 50.4% - val-roc: 0.6071 - val-ap: 0.5834 (0.1s/epoch)
Epoch  200/1000:Epoch loss: 0.3061 - avg acc: 50.4% - val-roc: 0.5867 - val-ap: 0.5729 (0.1s/epoch)
Epoch  201/1000:Epoch loss: 0.3462 - avg acc: 48.9% - val-roc: 0.6071 - val-ap: 0.5865 (0.1s/epoch)
Epoch  202/1000:Epoch loss: 0.3838 - avg acc: 62.6% - val-roc: 0.6071 - val-ap: 0.5805 (0.1s/epoch)
Epoch  203/1000:Epoch loss: 0.3560 - avg acc: 48.9% - val-roc: 0.5918 - val-ap: 0.5668 (0.1s/epoch)
Epoch  204/1000:Epoch loss: 0.3266 - avg acc: 43.5% - val-roc: 0.5765 - val-ap: 0.5477 (0.1s/epoch)
Epoch  205/1000:Epoch loss: 0.3431 - avg acc: 61.8% - val-roc: 0.5867 - val-ap: 0.5716 (0.1s/epoch)
Epoch  206/1000:Epoch loss: 0.3364 - avg acc: 44.3% - val-roc: 0.5663 - val-ap: 0.5655 (0.1s/epoch)
Epoch  207/1000:Epoch loss: 0.3509 - avg acc: 43.5% - val-roc: 0.5867 - val-ap: 0.5780 (0.1s/epoch)
Epoch  208/1000:Epoch loss: 0.2735 - avg acc: 63.4% - val-roc: 0.5816 - val-ap: 0.5793 (0.1s/epoch)
Epoch  209/1000:Epoch loss: 0.3593 - avg acc: 45.8% - val-roc: 0.5918 - val-ap: 0.5864 (0.1s/epoch)
Epoch  210/1000:Epoch loss: 0.3031 - avg acc: 56.5% - val-roc: 0.6276 - val-ap: 0.6082 (0.1s/epoch)
Epoch  211/1000:Epoch loss: 0.3235 - avg acc: 62.6% - val-roc: 0.6582 - val-ap: 0.6181 (0.1s/epoch)
Epoch  212/1000:Epoch loss: 0.3260 - avg acc: 61.1% - val-roc: 0.6735 - val-ap: 0.6256 (0.1s/epoch)
Epoch  213/1000:Epoch loss: 0.3444 - avg acc: 47.3% - val-roc: 0.6582 - val-ap: 0.6199 (0.1s/epoch)
Epoch  214/1000:Epoch loss: 0.3440 - avg acc: 48.9% - val-roc: 0.6224 - val-ap: 0.5965 (0.2s/epoch)
Epoch  215/1000:Epoch loss: 0.3041 - avg acc: 67.9% - val-roc: 0.6122 - val-ap: 0.5803 (0.1s/epoch)
Epoch  216/1000:Epoch loss: 0.4358 - avg acc: 58.8% - val-roc: 0.6122 - val-ap: 0.5719 (0.1s/epoch)
Epoch  217/1000:Epoch loss: 0.3034 - avg acc: 42.7% - val-roc: 0.4847 - val-ap: 0.4859 (0.1s/epoch)
Epoch  218/1000:Epoch loss: 0.3602 - avg acc: 48.1% - val-roc: 0.4694 - val-ap: 0.4888 (0.1s/epoch)
Epoch  219/1000:Epoch loss: 0.3318 - avg acc: 46.6% - val-roc: 0.5408 - val-ap: 0.5183 (0.1s/epoch)
Epoch  220/1000:Epoch loss: 0.3111 - avg acc: 61.8% - val-roc: 0.5765 - val-ap: 0.5335 (0.1s/epoch)
Epoch  221/1000:Epoch loss: 0.3091 - avg acc: 58.0% - val-roc: 0.6020 - val-ap: 0.5437 (0.1s/epoch)
Epoch  222/1000:Epoch loss: 0.3211 - avg acc: 43.5% - val-roc: 0.6224 - val-ap: 0.5786 (0.1s/epoch)
Epoch  223/1000:Epoch loss: 0.3368 - avg acc: 82.4% - val-roc: 0.6990 - val-ap: 0.6546 (0.1s/epoch)
Epoch  224/1000:Epoch loss: 0.3571 - avg acc: 61.1% - val-roc: 0.6939 - val-ap: 0.6589 (0.1s/epoch)
Epoch  225/1000:Epoch loss: 0.3849 - avg acc: 66.4% - val-roc: 0.7245 - val-ap: 0.6910 (0.1s/epoch)
Epoch  226/1000:Epoch loss: 0.3610 - avg acc: 50.4% - val-roc: 0.7041 - val-ap: 0.6699 (0.1s/epoch)
Epoch  227/1000:Epoch loss: 0.3339 - avg acc: 49.6% - val-roc: 0.6582 - val-ap: 0.6226 (0.1s/epoch)
Epoch  228/1000:Epoch loss: 0.3821 - avg acc: 49.6% - val-roc: 0.6582 - val-ap: 0.6160 (0.1s/epoch)
Epoch  229/1000:Epoch loss: 0.3072 - avg acc: 64.1% - val-roc: 0.7296 - val-ap: 0.6844 (0.1s/epoch)
Epoch  230/1000:Epoch loss: 0.3450 - avg acc: 61.8% - val-roc: 0.7449 - val-ap: 0.7012 (0.2s/epoch)
Epoch  231/1000:Epoch loss: 0.3385 - avg acc: 50.4% - val-roc: 0.7041 - val-ap: 0.6498 (0.1s/epoch)
Epoch  232/1000:Epoch loss: 0.3196 - avg acc: 62.6% - val-roc: 0.6684 - val-ap: 0.6424 (0.1s/epoch)
Epoch  233/1000:Epoch loss: 0.2823 - avg acc: 38.9% - val-roc: 0.5969 - val-ap: 0.5887 (0.1s/epoch)
Epoch  234/1000:Epoch loss: 0.2812 - avg acc: 84.0% - val-roc: 0.6888 - val-ap: 0.6505 (0.1s/epoch)
Epoch  235/1000:Epoch loss: 0.3516 - avg acc: 47.3% - val-roc: 0.6786 - val-ap: 0.6472 (0.1s/epoch)
Epoch  236/1000:Epoch loss: 0.3160 - avg acc: 62.6% - val-roc: 0.7245 - val-ap: 0.6840 (0.1s/epoch)
Epoch  237/1000:Epoch loss: 0.2833 - avg acc: 81.7% - val-roc: 0.7347 - val-ap: 0.6894 (0.1s/epoch)
Epoch  238/1000:Epoch loss: 0.3266 - avg acc: 47.3% - val-roc: 0.6837 - val-ap: 0.6325 (0.1s/epoch)
Epoch  239/1000:Epoch loss: 0.2755 - avg acc: 82.4% - val-roc: 0.6888 - val-ap: 0.6538 (0.1s/epoch)
Epoch  240/1000:Epoch loss: 0.3074 - avg acc: 48.9% - val-roc: 0.6480 - val-ap: 0.6191 (0.1s/epoch)
Epoch  241/1000:Epoch loss: 0.3417 - avg acc: 65.6% - val-roc: 0.6276 - val-ap: 0.6188 (0.1s/epoch)
Epoch  242/1000:Epoch loss: 0.2979 - avg acc: 64.9% - val-roc: 0.6378 - val-ap: 0.6390 (0.1s/epoch)
Epoch  243/1000:Epoch loss: 0.2790 - avg acc: 61.8% - val-roc: 0.6429 - val-ap: 0.6279 (0.1s/epoch)
Epoch  244/1000:Epoch loss: 0.2723 - avg acc: 83.2% - val-roc: 0.6684 - val-ap: 0.6454 (0.1s/epoch)
Epoch  245/1000:Epoch loss: 0.3059 - avg acc: 60.3% - val-roc: 0.6276 - val-ap: 0.6081 (0.1s/epoch)
Epoch  246/1000:Epoch loss: 0.3222 - avg acc: 63.4% - val-roc: 0.6327 - val-ap: 0.6587 (0.1s/epoch)
Epoch  247/1000:Epoch loss: 0.3132 - avg acc: 49.6% - val-roc: 0.5816 - val-ap: 0.5746 (0.1s/epoch)
Epoch  248/1000:Epoch loss: 0.3196 - avg acc: 62.6% - val-roc: 0.6276 - val-ap: 0.6098 (0.1s/epoch)
Epoch  249/1000:Epoch loss: 0.3223 - avg acc: 61.1% - val-roc: 0.6327 - val-ap: 0.6102 (0.1s/epoch)
Epoch  250/1000:Epoch loss: 0.3337 - avg acc: 64.1% - val-roc: 0.6429 - val-ap: 0.6268 (0.1s/epoch)
Epoch  251/1000:Epoch loss: 0.2927 - avg acc: 84.0% - val-roc: 0.6327 - val-ap: 0.6211 (0.1s/epoch)
Epoch  252/1000:Epoch loss: 0.3002 - avg acc: 50.4% - val-roc: 0.6020 - val-ap: 0.5918 (0.1s/epoch)
Epoch  253/1000:Epoch loss: 0.3207 - avg acc: 62.6% - val-roc: 0.6173 - val-ap: 0.6070 (0.1s/epoch)
Epoch  254/1000:Epoch loss: 0.2600 - avg acc: 61.8% - val-roc: 0.6582 - val-ap: 0.6357 (0.1s/epoch)
Epoch  255/1000:Epoch loss: 0.3532 - avg acc: 61.8% - val-roc: 0.6429 - val-ap: 0.6103 (0.1s/epoch)
Epoch  256/1000:Epoch loss: 0.3102 - avg acc: 62.6% - val-roc: 0.6480 - val-ap: 0.6119 (0.1s/epoch)
Epoch  257/1000:Epoch loss: 0.2840 - avg acc: 53.4% - val-roc: 0.6327 - val-ap: 0.5876 (0.1s/epoch)
Epoch  258/1000:Epoch loss: 0.2991 - avg acc: 53.4% - val-roc: 0.6071 - val-ap: 0.5751 (0.1s/epoch)
Epoch  259/1000:Epoch loss: 0.3623 - avg acc: 61.8% - val-roc: 0.6071 - val-ap: 0.5812 (0.1s/epoch)
Epoch  260/1000:Epoch loss: 0.2914 - avg acc: 64.1% - val-roc: 0.6071 - val-ap: 0.5833 (0.1s/epoch)
Epoch  261/1000:Epoch loss: 0.3066 - avg acc: 63.4% - val-roc: 0.6327 - val-ap: 0.5955 (0.1s/epoch)
Epoch  262/1000:Epoch loss: 0.2988 - avg acc: 83.2% - val-roc: 0.6735 - val-ap: 0.6431 (0.1s/epoch)
Epoch  263/1000:Epoch loss: 0.3284 - avg acc: 83.2% - val-roc: 0.6786 - val-ap: 0.6493 (0.1s/epoch)
Epoch  264/1000:Epoch loss: 0.2891 - avg acc: 86.3% - val-roc: 0.6888 - val-ap: 0.6677 (0.1s/epoch)
Epoch  265/1000:Epoch loss: 0.3360 - avg acc: 57.3% - val-roc: 0.6939 - val-ap: 0.6687 (0.1s/epoch)
Epoch  266/1000:Epoch loss: 0.2956 - avg acc: 87.0% - val-roc: 0.7041 - val-ap: 0.6801 (0.1s/epoch)
Epoch  267/1000:Epoch loss: 0.3240 - avg acc: 83.2% - val-roc: 0.6888 - val-ap: 0.6740 (0.1s/epoch)
Epoch  268/1000:Epoch loss: 0.3652 - avg acc: 62.6% - val-roc: 0.6837 - val-ap: 0.6677 (0.1s/epoch)
Epoch  269/1000:Epoch loss: 0.3485 - avg acc: 74.0% - val-roc: 0.7143 - val-ap: 0.6883 (0.1s/epoch)
Epoch  270/1000:Epoch loss: 0.3179 - avg acc: 48.9% - val-roc: 0.7347 - val-ap: 0.7001 (0.1s/epoch)
Epoch  271/1000:Epoch loss: 0.3287 - avg acc: 83.2% - val-roc: 0.7500 - val-ap: 0.7154 (0.2s/epoch)
Epoch  272/1000:Epoch loss: 0.2999 - avg acc: 63.4% - val-roc: 0.7092 - val-ap: 0.7136 (0.1s/epoch)
Epoch  273/1000:Epoch loss: 0.3842 - avg acc: 53.4% - val-roc: 0.6173 - val-ap: 0.6191 (0.1s/epoch)
Epoch  274/1000:Epoch loss: 0.2636 - avg acc: 50.4% - val-roc: 0.6020 - val-ap: 0.5710 (0.1s/epoch)
Epoch  275/1000:Epoch loss: 0.3254 - avg acc: 63.4% - val-roc: 0.6327 - val-ap: 0.6290 (0.1s/epoch)
Epoch  276/1000:Epoch loss: 0.3065 - avg acc: 83.2% - val-roc: 0.6684 - val-ap: 0.6563 (0.1s/epoch)
Epoch  277/1000:Epoch loss: 0.3171 - avg acc: 63.4% - val-roc: 0.6582 - val-ap: 0.6372 (0.1s/epoch)
Epoch  278/1000:Epoch loss: 0.2823 - avg acc: 51.9% - val-roc: 0.5867 - val-ap: 0.5828 (0.1s/epoch)
Epoch  279/1000:Epoch loss: 0.3566 - avg acc: 71.0% - val-roc: 0.5918 - val-ap: 0.5777 (0.1s/epoch)
Epoch  280/1000:Epoch loss: 0.2937 - avg acc: 53.4% - val-roc: 0.5765 - val-ap: 0.5665 (0.1s/epoch)
Epoch  281/1000:Epoch loss: 0.3282 - avg acc: 83.2% - val-roc: 0.6122 - val-ap: 0.5889 (0.1s/epoch)
Epoch  282/1000:Epoch loss: 0.2716 - avg acc: 63.4% - val-roc: 0.6633 - val-ap: 0.6515 (0.1s/epoch)
Epoch  283/1000:Epoch loss: 0.2997 - avg acc: 63.4% - val-roc: 0.6633 - val-ap: 0.6587 (0.1s/epoch)
Epoch  284/1000:Epoch loss: 0.3269 - avg acc: 67.2% - val-roc: 0.6582 - val-ap: 0.6546 (0.1s/epoch)
Epoch  285/1000:Epoch loss: 0.3219 - avg acc: 64.9% - val-roc: 0.6378 - val-ap: 0.6446 (0.1s/epoch)
Epoch  286/1000:Epoch loss: 0.3045 - avg acc: 61.8% - val-roc: 0.6378 - val-ap: 0.6434 (0.1s/epoch)
Epoch  287/1000:Epoch loss: 0.3553 - avg acc: 63.4% - val-roc: 0.6327 - val-ap: 0.6309 (0.1s/epoch)
Epoch  288/1000:Epoch loss: 0.2992 - avg acc: 63.4% - val-roc: 0.6020 - val-ap: 0.5936 (0.1s/epoch)
Epoch  289/1000:Epoch loss: 0.2868 - avg acc: 64.1% - val-roc: 0.5867 - val-ap: 0.6042 (0.1s/epoch)
Epoch  290/1000:Epoch loss: 0.3112 - avg acc: 66.4% - val-roc: 0.6633 - val-ap: 0.6590 (0.1s/epoch)
Epoch  291/1000:Epoch loss: 0.2974 - avg acc: 82.4% - val-roc: 0.6786 - val-ap: 0.6678 (0.1s/epoch)
Epoch  292/1000:Epoch loss: 0.3413 - avg acc: 83.2% - val-roc: 0.6990 - val-ap: 0.6800 (0.1s/epoch)
Epoch  293/1000:Epoch loss: 0.2560 - avg acc: 84.7% - val-roc: 0.7143 - val-ap: 0.6855 (0.1s/epoch)
Epoch  294/1000:Epoch loss: 0.2941 - avg acc: 61.8% - val-roc: 0.6939 - val-ap: 0.6658 (0.1s/epoch)
Epoch  295/1000:Epoch loss: 0.2826 - avg acc: 64.9% - val-roc: 0.6990 - val-ap: 0.7165 (0.1s/epoch)
Epoch  296/1000:Epoch loss: 0.3141 - avg acc: 61.1% - val-roc: 0.6786 - val-ap: 0.6938 (0.1s/epoch)
Epoch  297/1000:Epoch loss: 0.2544 - avg acc: 64.9% - val-roc: 0.6378 - val-ap: 0.6297 (0.1s/epoch)
Epoch  298/1000:Epoch loss: 0.3258 - avg acc: 52.7% - val-roc: 0.6276 - val-ap: 0.6022 (0.1s/epoch)
Epoch  299/1000:Epoch loss: 0.2456 - avg acc: 66.4% - val-roc: 0.6735 - val-ap: 0.6685 (0.1s/epoch)
Epoch  300/1000:Epoch loss: 0.2739 - avg acc: 63.4% - val-roc: 0.7041 - val-ap: 0.6772 (0.1s/epoch)
Epoch  301/1000:Epoch loss: 0.2697 - avg acc: 83.2% - val-roc: 0.7041 - val-ap: 0.6757 (0.1s/epoch)
Epoch  302/1000:Epoch loss: 0.2990 - avg acc: 63.4% - val-roc: 0.6837 - val-ap: 0.6679 (0.1s/epoch)
Epoch  303/1000:Epoch loss: 0.3054 - avg acc: 86.3% - val-roc: 0.6786 - val-ap: 0.6976 (0.1s/epoch)
Epoch  304/1000:Epoch loss: 0.2569 - avg acc: 60.3% - val-roc: 0.6633 - val-ap: 0.6710 (0.1s/epoch)
Epoch  305/1000:Epoch loss: 0.3331 - avg acc: 73.3% - val-roc: 0.6786 - val-ap: 0.6712 (0.1s/epoch)
Epoch  306/1000:Epoch loss: 0.2964 - avg acc: 71.0% - val-roc: 0.6888 - val-ap: 0.6481 (0.1s/epoch)
Epoch  307/1000:Epoch loss: 0.3446 - avg acc: 51.9% - val-roc: 0.6582 - val-ap: 0.6257 (0.1s/epoch)
Epoch  308/1000:Epoch loss: 0.3039 - avg acc: 85.5% - val-roc: 0.6633 - val-ap: 0.6373 (0.1s/epoch)
Epoch  309/1000:Epoch loss: 0.2941 - avg acc: 53.4% - val-roc: 0.6429 - val-ap: 0.6465 (0.1s/epoch)
Epoch  310/1000:Epoch loss: 0.3147 - avg acc: 87.0% - val-roc: 0.6224 - val-ap: 0.6202 (0.1s/epoch)
Epoch  311/1000:Epoch loss: 0.3073 - avg acc: 61.1% - val-roc: 0.6276 - val-ap: 0.6540 (0.1s/epoch)
Epoch  312/1000:Epoch loss: 0.2952 - avg acc: 84.0% - val-roc: 0.6888 - val-ap: 0.7265 (0.1s/epoch)
Epoch  313/1000:Epoch loss: 0.3095 - avg acc: 83.2% - val-roc: 0.7296 - val-ap: 0.7739 (0.2s/epoch)
Epoch  314/1000:Epoch loss: 0.2539 - avg acc: 85.5% - val-roc: 0.7347 - val-ap: 0.7743 (0.1s/epoch)
Epoch  315/1000:Epoch loss: 0.2959 - avg acc: 85.5% - val-roc: 0.7143 - val-ap: 0.7420 (0.1s/epoch)
Epoch  316/1000:Epoch loss: 0.2820 - avg acc: 87.0% - val-roc: 0.6837 - val-ap: 0.6919 (0.1s/epoch)
Epoch  317/1000:Epoch loss: 0.2916 - avg acc: 58.0% - val-roc: 0.6684 - val-ap: 0.6587 (0.1s/epoch)
Epoch  318/1000:Epoch loss: 0.2562 - avg acc: 88.5% - val-roc: 0.6582 - val-ap: 0.6545 (0.1s/epoch)
Epoch  319/1000:Epoch loss: 0.3195 - avg acc: 83.2% - val-roc: 0.6480 - val-ap: 0.6156 (0.1s/epoch)
Epoch  320/1000:Epoch loss: 0.3046 - avg acc: 84.0% - val-roc: 0.6531 - val-ap: 0.6576 (0.1s/epoch)
Epoch  321/1000:Epoch loss: 0.3277 - avg acc: 84.0% - val-roc: 0.6531 - val-ap: 0.6593 (0.1s/epoch)
Epoch  322/1000:Epoch loss: 0.3012 - avg acc: 63.4% - val-roc: 0.6071 - val-ap: 0.5978 (0.1s/epoch)
Epoch  323/1000:Epoch loss: 0.2755 - avg acc: 85.5% - val-roc: 0.6122 - val-ap: 0.6040 (0.1s/epoch)
Epoch  324/1000:Epoch loss: 0.3084 - avg acc: 64.1% - val-roc: 0.6633 - val-ap: 0.6345 (0.1s/epoch)
Epoch  325/1000:Epoch loss: 0.2611 - avg acc: 87.0% - val-roc: 0.7143 - val-ap: 0.7452 (0.1s/epoch)
Epoch  326/1000:Epoch loss: 0.3223 - avg acc: 83.2% - val-roc: 0.6939 - val-ap: 0.6751 (0.1s/epoch)
Epoch  327/1000:Epoch loss: 0.3741 - avg acc: 63.4% - val-roc: 0.6224 - val-ap: 0.6110 (0.1s/epoch)
Epoch  328/1000:Epoch loss: 0.3144 - avg acc: 71.8% - val-roc: 0.5918 - val-ap: 0.5864 (0.1s/epoch)
Epoch  329/1000:Epoch loss: 0.2930 - avg acc: 84.7% - val-roc: 0.6276 - val-ap: 0.6160 (0.1s/epoch)
Epoch  330/1000:Epoch loss: 0.2683 - avg acc: 66.4% - val-roc: 0.6224 - val-ap: 0.5936 (0.1s/epoch)
Epoch  331/1000:Epoch loss: 0.2806 - avg acc: 84.0% - val-roc: 0.6224 - val-ap: 0.5941 (0.1s/epoch)
Epoch  332/1000:Epoch loss: 0.3014 - avg acc: 84.0% - val-roc: 0.6173 - val-ap: 0.6040 (0.1s/epoch)
Epoch  333/1000:Epoch loss: 0.3301 - avg acc: 84.0% - val-roc: 0.6122 - val-ap: 0.5964 (0.1s/epoch)
Epoch  334/1000:Epoch loss: 0.2275 - avg acc: 64.1% - val-roc: 0.6173 - val-ap: 0.5863 (0.1s/epoch)
Epoch  335/1000:Epoch loss: 0.3263 - avg acc: 85.5% - val-roc: 0.6429 - val-ap: 0.6532 (0.1s/epoch)
Epoch  336/1000:Epoch loss: 0.2876 - avg acc: 64.9% - val-roc: 0.6531 - val-ap: 0.6606 (0.1s/epoch)
Epoch  337/1000:Epoch loss: 0.2755 - avg acc: 85.5% - val-roc: 0.6531 - val-ap: 0.6606 (0.1s/epoch)
Epoch  338/1000:Epoch loss: 0.2340 - avg acc: 64.9% - val-roc: 0.6173 - val-ap: 0.6020 (0.1s/epoch)
Epoch  339/1000:Epoch loss: 0.2773 - avg acc: 71.8% - val-roc: 0.6122 - val-ap: 0.5964 (0.1s/epoch)
Epoch  340/1000:Epoch loss: 0.2774 - avg acc: 84.0% - val-roc: 0.6429 - val-ap: 0.6301 (0.1s/epoch)
Epoch  341/1000:Epoch loss: 0.3015 - avg acc: 84.7% - val-roc: 0.6276 - val-ap: 0.6030 (0.1s/epoch)
Epoch  342/1000:Epoch loss: 0.2707 - avg acc: 74.0% - val-roc: 0.5867 - val-ap: 0.5756 (0.1s/epoch)
Epoch  343/1000:Epoch loss: 0.2937 - avg acc: 87.0% - val-roc: 0.5816 - val-ap: 0.5686 (0.1s/epoch)
Epoch  344/1000:Epoch loss: 0.3946 - avg acc: 84.0% - val-roc: 0.6173 - val-ap: 0.5980 (0.1s/epoch)
Epoch  345/1000:Epoch loss: 0.2532 - avg acc: 87.0% - val-roc: 0.6888 - val-ap: 0.6607 (0.1s/epoch)
Epoch  346/1000:Epoch loss: 0.2869 - avg acc: 87.0% - val-roc: 0.6990 - val-ap: 0.6654 (0.1s/epoch)
Epoch  347/1000:Epoch loss: 0.3225 - avg acc: 84.7% - val-roc: 0.6429 - val-ap: 0.6290 (0.1s/epoch)
Epoch  348/1000:Epoch loss: 0.2905 - avg acc: 73.3% - val-roc: 0.6122 - val-ap: 0.6181 (0.1s/epoch)
Epoch  349/1000:Epoch loss: 0.3098 - avg acc: 85.5% - val-roc: 0.6224 - val-ap: 0.6237 (0.1s/epoch)
Epoch  350/1000:Epoch loss: 0.2841 - avg acc: 87.0% - val-roc: 0.6429 - val-ap: 0.6343 (0.1s/epoch)
Epoch  351/1000:Epoch loss: 0.2379 - avg acc: 82.4% - val-roc: 0.6633 - val-ap: 0.6502 (0.1s/epoch)
Epoch  352/1000:Epoch loss: 0.2708 - avg acc: 86.3% - val-roc: 0.6786 - val-ap: 0.6643 (0.1s/epoch)
Epoch  353/1000:Epoch loss: 0.2610 - avg acc: 83.2% - val-roc: 0.6837 - val-ap: 0.7000 (0.1s/epoch)
Epoch  354/1000:Epoch loss: 0.3201 - avg acc: 85.5% - val-roc: 0.6786 - val-ap: 0.6837 (0.1s/epoch)
Epoch  355/1000:Epoch loss: 0.2977 - avg acc: 87.0% - val-roc: 0.6582 - val-ap: 0.6381 (0.1s/epoch)
Epoch  356/1000:Epoch loss: 0.2769 - avg acc: 64.1% - val-roc: 0.6122 - val-ap: 0.6168 (0.1s/epoch)
Epoch  357/1000:Epoch loss: 0.2939 - avg acc: 87.8% - val-roc: 0.5765 - val-ap: 0.5920 (0.1s/epoch)
Epoch  358/1000:Epoch loss: 0.2909 - avg acc: 64.9% - val-roc: 0.5765 - val-ap: 0.5896 (0.1s/epoch)
Epoch  359/1000:Epoch loss: 0.2622 - avg acc: 87.8% - val-roc: 0.6173 - val-ap: 0.6133 (0.1s/epoch)
Epoch  360/1000:Epoch loss: 0.2738 - avg acc: 61.8% - val-roc: 0.6633 - val-ap: 0.6741 (0.1s/epoch)
Epoch  361/1000:Epoch loss: 0.2612 - avg acc: 87.0% - val-roc: 0.6633 - val-ap: 0.6769 (0.1s/epoch)
Epoch  362/1000:Epoch loss: 0.2698 - avg acc: 87.0% - val-roc: 0.6633 - val-ap: 0.6837 (0.1s/epoch)
Epoch  363/1000:Epoch loss: 0.3020 - avg acc: 64.1% - val-roc: 0.6122 - val-ap: 0.6135 (0.1s/epoch)
Epoch  364/1000:Epoch loss: 0.2506 - avg acc: 86.3% - val-roc: 0.6122 - val-ap: 0.6135 (0.1s/epoch)
Epoch  365/1000:Epoch loss: 0.3277 - avg acc: 87.0% - val-roc: 0.5867 - val-ap: 0.5918 (0.1s/epoch)
Epoch  366/1000:Epoch loss: 0.2766 - avg acc: 65.6% - val-roc: 0.5765 - val-ap: 0.5811 (0.1s/epoch)
Epoch  367/1000:Epoch loss: 0.2784 - avg acc: 87.0% - val-roc: 0.5612 - val-ap: 0.5453 (0.1s/epoch)
Epoch  368/1000:Epoch loss: 0.2838 - avg acc: 86.3% - val-roc: 0.5612 - val-ap: 0.5528 (0.1s/epoch)
Epoch  369/1000:Epoch loss: 0.2973 - avg acc: 83.2% - val-roc: 0.5663 - val-ap: 0.5573 (0.1s/epoch)
Epoch  370/1000:Epoch loss: 0.3113 - avg acc: 87.0% - val-roc: 0.5867 - val-ap: 0.5910 (0.1s/epoch)
Epoch  371/1000:Epoch loss: 0.3282 - avg acc: 84.0% - val-roc: 0.5765 - val-ap: 0.5627 (0.1s/epoch)
Epoch  372/1000:Epoch loss: 0.2639 - avg acc: 87.8% - val-roc: 0.5663 - val-ap: 0.5598 (0.1s/epoch)
Epoch  373/1000:Epoch loss: 0.2599 - avg acc: 87.0% - val-roc: 0.5714 - val-ap: 0.5588 (0.1s/epoch)
Epoch  374/1000:Epoch loss: 0.3115 - avg acc: 85.5% - val-roc: 0.5867 - val-ap: 0.5906 (0.1s/epoch)
Epoch  375/1000:Epoch loss: 0.2871 - avg acc: 84.7% - val-roc: 0.5969 - val-ap: 0.5944 (0.1s/epoch)
Epoch  376/1000:Epoch loss: 0.2557 - avg acc: 89.3% - val-roc: 0.6480 - val-ap: 0.6456 (0.1s/epoch)
Epoch  377/1000:Epoch loss: 0.2768 - avg acc: 72.5% - val-roc: 0.6071 - val-ap: 0.5879 (0.1s/epoch)
Epoch  378/1000:Epoch loss: 0.2832 - avg acc: 89.3% - val-roc: 0.6173 - val-ap: 0.6025 (0.1s/epoch)
Epoch  379/1000:Epoch loss: 0.3011 - avg acc: 73.3% - val-roc: 0.5765 - val-ap: 0.5606 (0.1s/epoch)
Epoch  380/1000:Epoch loss: 0.2803 - avg acc: 86.3% - val-roc: 0.5663 - val-ap: 0.5537 (0.1s/epoch)
Epoch  381/1000:Epoch loss: 0.3665 - avg acc: 66.4% - val-roc: 0.5459 - val-ap: 0.5446 (0.1s/epoch)
Epoch  382/1000:Epoch loss: 0.3039 - avg acc: 87.8% - val-roc: 0.5918 - val-ap: 0.5929 (0.1s/epoch)
Epoch  383/1000:Epoch loss: 0.2796 - avg acc: 87.8% - val-roc: 0.6020 - val-ap: 0.5983 (0.1s/epoch)
Epoch  384/1000:Epoch loss: 0.2848 - avg acc: 85.5% - val-roc: 0.6122 - val-ap: 0.6085 (0.1s/epoch)
Epoch  385/1000:Epoch loss: 0.2437 - avg acc: 88.5% - val-roc: 0.6173 - val-ap: 0.6172 (0.1s/epoch)
Epoch  386/1000:Epoch loss: 0.2493 - avg acc: 86.3% - val-roc: 0.5918 - val-ap: 0.5883 (0.1s/epoch)
Epoch  387/1000:Epoch loss: 0.2645 - avg acc: 87.0% - val-roc: 0.6327 - val-ap: 0.6232 (0.1s/epoch)
Epoch  388/1000:Epoch loss: 0.2801 - avg acc: 88.5% - val-roc: 0.6582 - val-ap: 0.6403 (0.1s/epoch)
Epoch  389/1000:Epoch loss: 0.2858 - avg acc: 86.3% - val-roc: 0.6582 - val-ap: 0.7015 (0.1s/epoch)
Epoch  390/1000:Epoch loss: 0.2846 - avg acc: 88.5% - val-roc: 0.6735 - val-ap: 0.7223 (0.1s/epoch)
Epoch  391/1000:Epoch loss: 0.2828 - avg acc: 83.2% - val-roc: 0.6429 - val-ap: 0.6966 (0.1s/epoch)
Epoch  392/1000:Epoch loss: 0.3090 - avg acc: 89.3% - val-roc: 0.6786 - val-ap: 0.7190 (0.1s/epoch)
Epoch  393/1000:Epoch loss: 0.2811 - avg acc: 87.0% - val-roc: 0.6582 - val-ap: 0.6763 (0.1s/epoch)
Epoch  394/1000:Epoch loss: 0.2788 - avg acc: 85.5% - val-roc: 0.6378 - val-ap: 0.6241 (0.1s/epoch)
Epoch  395/1000:Epoch loss: 0.3040 - avg acc: 85.5% - val-roc: 0.6633 - val-ap: 0.6871 (0.1s/epoch)
Epoch  396/1000:Epoch loss: 0.2728 - avg acc: 87.8% - val-roc: 0.6735 - val-ap: 0.7116 (0.1s/epoch)
Epoch  397/1000:Epoch loss: 0.3102 - avg acc: 87.0% - val-roc: 0.6429 - val-ap: 0.6197 (0.1s/epoch)
Epoch  398/1000:Epoch loss: 0.2638 - avg acc: 88.5% - val-roc: 0.6327 - val-ap: 0.6210 (0.1s/epoch)
Epoch  399/1000:Epoch loss: 0.2614 - avg acc: 86.3% - val-roc: 0.5765 - val-ap: 0.5793 (0.1s/epoch)
Epoch  400/1000:Epoch loss: 0.2756 - avg acc: 84.7% - val-roc: 0.6224 - val-ap: 0.6630 (0.1s/epoch)
Epoch  401/1000:Epoch loss: 0.2985 - avg acc: 85.5% - val-roc: 0.6122 - val-ap: 0.6057 (0.1s/epoch)
Epoch  402/1000:Epoch loss: 0.3128 - avg acc: 88.5% - val-roc: 0.6786 - val-ap: 0.7274 (0.1s/epoch)
Epoch  403/1000:Epoch loss: 0.3154 - avg acc: 86.3% - val-roc: 0.6735 - val-ap: 0.7022 (0.1s/epoch)
Epoch  404/1000:Epoch loss: 0.3255 - avg acc: 87.0% - val-roc: 0.6684 - val-ap: 0.7068 (0.1s/epoch)
Epoch  405/1000:Epoch loss: 0.2643 - avg acc: 88.5% - val-roc: 0.6582 - val-ap: 0.7013 (0.1s/epoch)
Epoch  406/1000:Epoch loss: 0.3300 - avg acc: 84.7% - val-roc: 0.6582 - val-ap: 0.6339 (0.1s/epoch)
Epoch  407/1000:Epoch loss: 0.2299 - avg acc: 88.5% - val-roc: 0.6429 - val-ap: 0.6335 (0.1s/epoch)
Epoch  408/1000:Epoch loss: 0.2626 - avg acc: 66.4% - val-roc: 0.6071 - val-ap: 0.6076 (0.1s/epoch)
Epoch  409/1000:Epoch loss: 0.3190 - avg acc: 88.5% - val-roc: 0.6480 - val-ap: 0.6337 (0.1s/epoch)
Epoch  410/1000:Epoch loss: 0.2762 - avg acc: 72.5% - val-roc: 0.6633 - val-ap: 0.6512 (0.1s/epoch)
Epoch  411/1000:Epoch loss: 0.2649 - avg acc: 87.8% - val-roc: 0.6990 - val-ap: 0.6755 (0.1s/epoch)
Epoch  412/1000:Epoch loss: 0.2543 - avg acc: 87.0% - val-roc: 0.6786 - val-ap: 0.6586 (0.1s/epoch)
Epoch  413/1000:Epoch loss: 0.3289 - avg acc: 86.3% - val-roc: 0.6684 - val-ap: 0.6490 (0.1s/epoch)
Epoch  414/1000:Epoch loss: 0.2992 - avg acc: 86.3% - val-roc: 0.6531 - val-ap: 0.6336 (0.1s/epoch)
Epoch  415/1000:Epoch loss: 0.3009 - avg acc: 87.8% - val-roc: 0.6531 - val-ap: 0.6424 (0.1s/epoch)
Epoch  416/1000:Epoch loss: 0.3036 - avg acc: 86.3% - val-roc: 0.6327 - val-ap: 0.6210 (0.1s/epoch)
Epoch  417/1000:Epoch loss: 0.3214 - avg acc: 87.0% - val-roc: 0.6837 - val-ap: 0.6519 (0.1s/epoch)
Epoch  418/1000:Epoch loss: 0.3063 - avg acc: 84.7% - val-roc: 0.6480 - val-ap: 0.6342 (0.1s/epoch)
Epoch  419/1000:Epoch loss: 0.2916 - avg acc: 85.5% - val-roc: 0.6276 - val-ap: 0.6258 (0.1s/epoch)
Epoch  420/1000:Epoch loss: 0.2701 - avg acc: 89.3% - val-roc: 0.6735 - val-ap: 0.6591 (0.1s/epoch)
Epoch  421/1000:Epoch loss: 0.3002 - avg acc: 87.0% - val-roc: 0.7245 - val-ap: 0.7728 (0.1s/epoch)
Epoch  422/1000:Epoch loss: 0.2619 - avg acc: 87.0% - val-roc: 0.7347 - val-ap: 0.7688 (0.1s/epoch)
Epoch  423/1000:Epoch loss: 0.2905 - avg acc: 88.5% - val-roc: 0.6735 - val-ap: 0.7098 (0.1s/epoch)
Epoch  424/1000:Epoch loss: 0.2865 - avg acc: 87.0% - val-roc: 0.6378 - val-ap: 0.6226 (0.1s/epoch)
Epoch  425/1000:Epoch loss: 0.2964 - avg acc: 88.5% - val-roc: 0.6531 - val-ap: 0.6331 (0.1s/epoch)
Epoch  426/1000:Epoch loss: 0.2669 - avg acc: 62.6% - val-roc: 0.6531 - val-ap: 0.6467 (0.1s/epoch)
Epoch  427/1000:Epoch loss: 0.3032 - avg acc: 90.8% - val-roc: 0.7143 - val-ap: 0.7399 (0.1s/epoch)
Epoch  428/1000:Epoch loss: 0.3080 - avg acc: 69.5% - val-roc: 0.6327 - val-ap: 0.5976 (0.1s/epoch)
Epoch  429/1000:Epoch loss: 0.3022 - avg acc: 87.0% - val-roc: 0.6020 - val-ap: 0.5819 (0.1s/epoch)
Epoch  430/1000:Epoch loss: 0.2419 - avg acc: 80.9% - val-roc: 0.6122 - val-ap: 0.5876 (0.1s/epoch)
Epoch  431/1000:Epoch loss: 0.2604 - avg acc: 86.3% - val-roc: 0.6327 - val-ap: 0.6179 (0.1s/epoch)
Epoch  432/1000:Epoch loss: 0.3341 - avg acc: 87.8% - val-roc: 0.6735 - val-ap: 0.6497 (0.1s/epoch)
Epoch  433/1000:Epoch loss: 0.3120 - avg acc: 55.7% - val-roc: 0.6735 - val-ap: 0.7126 (0.1s/epoch)
Epoch  434/1000:Epoch loss: 0.3058 - avg acc: 87.8% - val-roc: 0.7092 - val-ap: 0.7429 (0.1s/epoch)
Epoch  435/1000:Epoch loss: 0.2793 - avg acc: 85.5% - val-roc: 0.6735 - val-ap: 0.6828 (0.2s/epoch)
Epoch  436/1000:Epoch loss: 0.2481 - avg acc: 85.5% - val-roc: 0.6582 - val-ap: 0.6415 (0.1s/epoch)
Epoch  437/1000:Epoch loss: 0.2176 - avg acc: 87.8% - val-roc: 0.6582 - val-ap: 0.6422 (0.1s/epoch)
Epoch  438/1000:Epoch loss: 0.2721 - avg acc: 87.8% - val-roc: 0.6939 - val-ap: 0.6708 (0.1s/epoch)
Epoch  439/1000:Epoch loss: 0.2888 - avg acc: 88.5% - val-roc: 0.7143 - val-ap: 0.6792 (0.1s/epoch)
Epoch  440/1000:Epoch loss: 0.2710 - avg acc: 86.3% - val-roc: 0.6888 - val-ap: 0.6375 (0.1s/epoch)
Epoch  441/1000:Epoch loss: 0.2656 - avg acc: 88.5% - val-roc: 0.7245 - val-ap: 0.6776 (0.1s/epoch)
Epoch  442/1000:Epoch loss: 0.2798 - avg acc: 87.0% - val-roc: 0.7245 - val-ap: 0.6845 (0.1s/epoch)
Epoch  443/1000:Epoch loss: 0.3162 - avg acc: 87.0% - val-roc: 0.7296 - val-ap: 0.6895 (0.1s/epoch)
Epoch  444/1000:Epoch loss: 0.2557 - avg acc: 88.5% - val-roc: 0.6990 - val-ap: 0.6617 (0.1s/epoch)
Epoch  445/1000:Epoch loss: 0.2619 - avg acc: 85.5% - val-roc: 0.6939 - val-ap: 0.6525 (0.2s/epoch)
Epoch  446/1000:Epoch loss: 0.2294 - avg acc: 88.5% - val-roc: 0.7500 - val-ap: 0.7694 (0.1s/epoch)
Epoch  447/1000:Epoch loss: 0.2997 - avg acc: 87.0% - val-roc: 0.7143 - val-ap: 0.7277 (0.1s/epoch)
Epoch  448/1000:Epoch loss: 0.3703 - avg acc: 88.5% - val-roc: 0.7041 - val-ap: 0.7337 (0.1s/epoch)
Epoch  449/1000:Epoch loss: 0.3122 - avg acc: 89.3% - val-roc: 0.6633 - val-ap: 0.6681 (0.1s/epoch)
Epoch  450/1000:Epoch loss: 0.2492 - avg acc: 81.7% - val-roc: 0.6378 - val-ap: 0.6492 (0.1s/epoch)
Epoch  451/1000:Epoch loss: 0.2811 - avg acc: 88.5% - val-roc: 0.6327 - val-ap: 0.6114 (0.1s/epoch)
Epoch  452/1000:Epoch loss: 0.2659 - avg acc: 87.0% - val-roc: 0.6429 - val-ap: 0.6614 (0.1s/epoch)
Epoch  453/1000:Epoch loss: 0.2542 - avg acc: 88.5% - val-roc: 0.6224 - val-ap: 0.6398 (0.1s/epoch)
Epoch  454/1000:Epoch loss: 0.2265 - avg acc: 88.5% - val-roc: 0.6327 - val-ap: 0.6500 (0.1s/epoch)
Epoch  455/1000:Epoch loss: 0.2977 - avg acc: 89.3% - val-roc: 0.6224 - val-ap: 0.6120 (0.2s/epoch)
Epoch  456/1000:Epoch loss: 0.2736 - avg acc: 87.8% - val-roc: 0.6378 - val-ap: 0.6779 (0.1s/epoch)
Epoch  457/1000:Epoch loss: 0.2672 - avg acc: 87.8% - val-roc: 0.6378 - val-ap: 0.6789 (0.1s/epoch)
Epoch  458/1000:Epoch loss: 0.2818 - avg acc: 88.5% - val-roc: 0.6480 - val-ap: 0.6749 (0.1s/epoch)
Epoch  459/1000:Epoch loss: 0.2819 - avg acc: 87.8% - val-roc: 0.6276 - val-ap: 0.6087 (0.1s/epoch)
Epoch  460/1000:Epoch loss: 0.2939 - avg acc: 89.3% - val-roc: 0.6173 - val-ap: 0.6083 (0.1s/epoch)
Epoch  461/1000:Epoch loss: 0.2397 - avg acc: 87.0% - val-roc: 0.6122 - val-ap: 0.6094 (0.1s/epoch)
Epoch  462/1000:Epoch loss: 0.2519 - avg acc: 88.5% - val-roc: 0.6531 - val-ap: 0.6398 (0.1s/epoch)
Epoch  463/1000:Epoch loss: 0.2570 - avg acc: 88.5% - val-roc: 0.6531 - val-ap: 0.6382 (0.1s/epoch)
Epoch  464/1000:Epoch loss: 0.2500 - avg acc: 88.5% - val-roc: 0.6224 - val-ap: 0.6639 (0.1s/epoch)
Epoch  465/1000:Epoch loss: 0.2990 - avg acc: 88.5% - val-roc: 0.5663 - val-ap: 0.5728 (0.1s/epoch)
Epoch  466/1000:Epoch loss: 0.2137 - avg acc: 64.9% - val-roc: 0.5408 - val-ap: 0.5414 (0.1s/epoch)
Epoch  467/1000:Epoch loss: 0.2405 - avg acc: 89.3% - val-roc: 0.6122 - val-ap: 0.6078 (0.1s/epoch)
Epoch  468/1000:Epoch loss: 0.2182 - avg acc: 87.8% - val-roc: 0.6735 - val-ap: 0.7129 (0.1s/epoch)
Epoch  469/1000:Epoch loss: 0.2671 - avg acc: 88.5% - val-roc: 0.6888 - val-ap: 0.7131 (0.1s/epoch)
Epoch  470/1000:Epoch loss: 0.2839 - avg acc: 88.5% - val-roc: 0.7041 - val-ap: 0.6725 (0.1s/epoch)
Epoch  471/1000:Epoch loss: 0.3318 - avg acc: 87.0% - val-roc: 0.6735 - val-ap: 0.6581 (0.1s/epoch)
Epoch  472/1000:Epoch loss: 0.2674 - avg acc: 87.8% - val-roc: 0.6837 - val-ap: 0.6612 (0.1s/epoch)
Epoch  473/1000:Epoch loss: 0.2705 - avg acc: 87.0% - val-roc: 0.6582 - val-ap: 0.6532 (0.1s/epoch)
Epoch  474/1000:Epoch loss: 0.2196 - avg acc: 88.5% - val-roc: 0.6990 - val-ap: 0.7458 (0.1s/epoch)
Epoch  475/1000:Epoch loss: 0.2303 - avg acc: 89.3% - val-roc: 0.6633 - val-ap: 0.6827 (0.1s/epoch)
Epoch  476/1000:Epoch loss: 0.2518 - avg acc: 88.5% - val-roc: 0.6276 - val-ap: 0.6496 (0.1s/epoch)
Epoch  477/1000:Epoch loss: 0.2264 - avg acc: 88.5% - val-roc: 0.6684 - val-ap: 0.7179 (0.1s/epoch)
Epoch  478/1000:Epoch loss: 0.3197 - avg acc: 88.5% - val-roc: 0.6735 - val-ap: 0.7146 (0.1s/epoch)
Epoch  479/1000:Epoch loss: 0.2773 - avg acc: 88.5% - val-roc: 0.6735 - val-ap: 0.7238 (0.1s/epoch)
Epoch  480/1000:Epoch loss: 0.2917 - avg acc: 88.5% - val-roc: 0.6888 - val-ap: 0.7345 (0.1s/epoch)
Epoch  481/1000:Epoch loss: 0.3020 - avg acc: 87.0% - val-roc: 0.6786 - val-ap: 0.7247 (0.1s/epoch)
Epoch  482/1000:Epoch loss: 0.2483 - avg acc: 89.3% - val-roc: 0.6939 - val-ap: 0.7318 (0.1s/epoch)
Epoch  483/1000:Epoch loss: 0.2653 - avg acc: 88.5% - val-roc: 0.6633 - val-ap: 0.7042 (0.1s/epoch)
Epoch  484/1000:Epoch loss: 0.2910 - avg acc: 89.3% - val-roc: 0.6582 - val-ap: 0.6754 (0.1s/epoch)
Epoch  485/1000:Epoch loss: 0.2941 - avg acc: 68.7% - val-roc: 0.6429 - val-ap: 0.6903 (0.1s/epoch)
Epoch  486/1000:Epoch loss: 0.2354 - avg acc: 89.3% - val-roc: 0.6786 - val-ap: 0.7299 (0.1s/epoch)
Epoch  487/1000:Epoch loss: 0.2664 - avg acc: 88.5% - val-roc: 0.6531 - val-ap: 0.7046 (0.1s/epoch)
Epoch  488/1000:Epoch loss: 0.2529 - avg acc: 89.3% - val-roc: 0.6684 - val-ap: 0.7206 (0.1s/epoch)
Epoch  489/1000:Epoch loss: 0.2013 - avg acc: 68.7% - val-roc: 0.6531 - val-ap: 0.7094 (0.1s/epoch)
Epoch  490/1000:Epoch loss: 0.2781 - avg acc: 90.1% - val-roc: 0.6786 - val-ap: 0.7200 (0.1s/epoch)
Epoch  491/1000:Epoch loss: 0.2543 - avg acc: 84.7% - val-roc: 0.6480 - val-ap: 0.6935 (0.1s/epoch)
Epoch  492/1000:Epoch loss: 0.2937 - avg acc: 89.3% - val-roc: 0.6990 - val-ap: 0.7300 (0.1s/epoch)
Epoch  493/1000:Epoch loss: 0.2907 - avg acc: 89.3% - val-roc: 0.7194 - val-ap: 0.7529 (0.1s/epoch)
Epoch  494/1000:Epoch loss: 0.2757 - avg acc: 88.5% - val-roc: 0.6888 - val-ap: 0.7124 (0.1s/epoch)
Epoch  495/1000:Epoch loss: 0.2481 - avg acc: 89.3% - val-roc: 0.6633 - val-ap: 0.6439 (0.1s/epoch)
Epoch  496/1000:Epoch loss: 0.2861 - avg acc: 88.5% - val-roc: 0.6480 - val-ap: 0.6234 (0.1s/epoch)
Epoch  497/1000:Epoch loss: 0.2771 - avg acc: 87.8% - val-roc: 0.6888 - val-ap: 0.6529 (0.1s/epoch)
Epoch  498/1000:Epoch loss: 0.2782 - avg acc: 89.3% - val-roc: 0.6888 - val-ap: 0.6492 (0.1s/epoch)
Epoch  499/1000:Epoch loss: 0.2421 - avg acc: 83.2% - val-roc: 0.6224 - val-ap: 0.5824 (0.1s/epoch)
Epoch  500/1000:Epoch loss: 0.2759 - avg acc: 90.1% - val-roc: 0.6122 - val-ap: 0.5850 (0.1s/epoch)
Epoch  501/1000:Epoch loss: 0.2695 - avg acc: 87.8% - val-roc: 0.6327 - val-ap: 0.6127 (0.1s/epoch)
Epoch  502/1000:Epoch loss: 0.2201 - avg acc: 87.0% - val-roc: 0.6684 - val-ap: 0.6450 (0.1s/epoch)
Epoch  503/1000:Epoch loss: 0.3428 - avg acc: 90.8% - val-roc: 0.6837 - val-ap: 0.6656 (0.1s/epoch)
Epoch  504/1000:Epoch loss: 0.3068 - avg acc: 80.9% - val-roc: 0.6684 - val-ap: 0.6533 (0.1s/epoch)
Epoch  505/1000:Epoch loss: 0.2892 - avg acc: 90.1% - val-roc: 0.6837 - val-ap: 0.6611 (0.1s/epoch)
Epoch  506/1000:Epoch loss: 0.2122 - avg acc: 80.9% - val-roc: 0.6531 - val-ap: 0.6549 (0.1s/epoch)
Epoch  507/1000:Epoch loss: 0.2526 - avg acc: 90.1% - val-roc: 0.6735 - val-ap: 0.6769 (0.1s/epoch)
Epoch  508/1000:Epoch loss: 0.3503 - avg acc: 88.5% - val-roc: 0.6122 - val-ap: 0.5998 (0.1s/epoch)
Epoch  509/1000:Epoch loss: 0.2759 - avg acc: 89.3% - val-roc: 0.6378 - val-ap: 0.6126 (0.1s/epoch)
Epoch  510/1000:Epoch loss: 0.2421 - avg acc: 88.5% - val-roc: 0.6276 - val-ap: 0.6036 (0.1s/epoch)
Epoch  511/1000:Epoch loss: 0.2524 - avg acc: 89.3% - val-roc: 0.6327 - val-ap: 0.6068 (0.1s/epoch)
Epoch  512/1000:Epoch loss: 0.2153 - avg acc: 89.3% - val-roc: 0.6173 - val-ap: 0.6004 (0.1s/epoch)
Epoch  513/1000:Epoch loss: 0.2162 - avg acc: 90.1% - val-roc: 0.6071 - val-ap: 0.5949 (0.1s/epoch)
Epoch  514/1000:Epoch loss: 0.2380 - avg acc: 90.1% - val-roc: 0.6173 - val-ap: 0.5983 (0.1s/epoch)
Epoch  515/1000:Epoch loss: 0.2735 - avg acc: 90.1% - val-roc: 0.6327 - val-ap: 0.6062 (0.1s/epoch)
Epoch  516/1000:Epoch loss: 0.2559 - avg acc: 89.3% - val-roc: 0.6224 - val-ap: 0.6158 (0.1s/epoch)
Epoch  517/1000:Epoch loss: 0.2611 - avg acc: 91.6% - val-roc: 0.6327 - val-ap: 0.6125 (0.1s/epoch)
Epoch  518/1000:Epoch loss: 0.3123 - avg acc: 83.2% - val-roc: 0.6378 - val-ap: 0.6095 (0.1s/epoch)
Epoch  519/1000:Epoch loss: 0.3074 - avg acc: 89.3% - val-roc: 0.6531 - val-ap: 0.6340 (0.1s/epoch)
Epoch  520/1000:Epoch loss: 0.2113 - avg acc: 90.1% - val-roc: 0.6633 - val-ap: 0.6459 (0.1s/epoch)
Epoch  521/1000:Epoch loss: 0.2455 - avg acc: 88.5% - val-roc: 0.6276 - val-ap: 0.6123 (0.1s/epoch)
Epoch  522/1000:Epoch loss: 0.2341 - avg acc: 90.1% - val-roc: 0.6071 - val-ap: 0.6031 (0.1s/epoch)
Epoch  523/1000:Epoch loss: 0.2106 - avg acc: 87.8% - val-roc: 0.6173 - val-ap: 0.6201 (0.1s/epoch)
Epoch  524/1000:Epoch loss: 0.2453 - avg acc: 90.1% - val-roc: 0.6429 - val-ap: 0.6361 (0.1s/epoch)
Epoch  525/1000:Epoch loss: 0.3352 - avg acc: 87.8% - val-roc: 0.6122 - val-ap: 0.6191 (0.1s/epoch)
Epoch  526/1000:Epoch loss: 0.2660 - avg acc: 88.5% - val-roc: 0.5816 - val-ap: 0.5602 (0.1s/epoch)
Epoch  527/1000:Epoch loss: 0.2504 - avg acc: 90.1% - val-roc: 0.6020 - val-ap: 0.5697 (0.1s/epoch)
Epoch  528/1000:Epoch loss: 0.2646 - avg acc: 90.1% - val-roc: 0.6224 - val-ap: 0.5835 (0.1s/epoch)
Epoch  529/1000:Epoch loss: 0.2356 - avg acc: 90.8% - val-roc: 0.6531 - val-ap: 0.6240 (0.1s/epoch)
Epoch  530/1000:Epoch loss: 0.2800 - avg acc: 88.5% - val-roc: 0.6582 - val-ap: 0.6310 (0.1s/epoch)
Epoch  531/1000:Epoch loss: 0.2879 - avg acc: 89.3% - val-roc: 0.6888 - val-ap: 0.6616 (0.1s/epoch)
Epoch  532/1000:Epoch loss: 0.2962 - avg acc: 88.5% - val-roc: 0.6990 - val-ap: 0.6686 (0.1s/epoch)
Epoch  533/1000:Epoch loss: 0.2487 - avg acc: 90.8% - val-roc: 0.6633 - val-ap: 0.6425 (0.1s/epoch)
Epoch  534/1000:Epoch loss: 0.2725 - avg acc: 89.3% - val-roc: 0.6939 - val-ap: 0.6661 (0.1s/epoch)
Epoch  535/1000:Epoch loss: 0.2327 - avg acc: 89.3% - val-roc: 0.6786 - val-ap: 0.6557 (0.1s/epoch)
Epoch  536/1000:Epoch loss: 0.2165 - avg acc: 85.5% - val-roc: 0.6582 - val-ap: 0.6454 (0.1s/epoch)
Epoch  537/1000:Epoch loss: 0.2350 - avg acc: 89.3% - val-roc: 0.6939 - val-ap: 0.6701 (0.1s/epoch)
Epoch  538/1000:Epoch loss: 0.2280 - avg acc: 89.3% - val-roc: 0.6939 - val-ap: 0.7132 (0.1s/epoch)
Epoch  539/1000:Epoch loss: 0.2220 - avg acc: 89.3% - val-roc: 0.7398 - val-ap: 0.7700 (0.1s/epoch)
Epoch  540/1000:Epoch loss: 0.2552 - avg acc: 90.1% - val-roc: 0.7398 - val-ap: 0.7676 (0.1s/epoch)
Epoch  541/1000:Epoch loss: 0.2805 - avg acc: 90.1% - val-roc: 0.7041 - val-ap: 0.6777 (0.1s/epoch)
Epoch  542/1000:Epoch loss: 0.2504 - avg acc: 90.8% - val-roc: 0.7194 - val-ap: 0.6842 (0.1s/epoch)
Epoch  543/1000:Epoch loss: 0.2316 - avg acc: 90.1% - val-roc: 0.7347 - val-ap: 0.7052 (0.1s/epoch)
Epoch  544/1000:Epoch loss: 0.2099 - avg acc: 90.8% - val-roc: 0.7755 - val-ap: 0.8084 (0.1s/epoch)
Epoch  545/1000:Epoch loss: 0.1907 - avg acc: 90.1% - val-roc: 0.7806 - val-ap: 0.8101 (0.1s/epoch)
Epoch  546/1000:Epoch loss: 0.3093 - avg acc: 89.3% - val-roc: 0.7857 - val-ap: 0.8244 (0.1s/epoch)
Epoch  547/1000:Epoch loss: 0.2442 - avg acc: 91.6% - val-roc: 0.6990 - val-ap: 0.6699 (0.1s/epoch)
Epoch  548/1000:Epoch loss: 0.2246 - avg acc: 90.8% - val-roc: 0.6684 - val-ap: 0.6453 (0.1s/epoch)
Epoch  549/1000:Epoch loss: 0.2398 - avg acc: 90.8% - val-roc: 0.6531 - val-ap: 0.6294 (0.1s/epoch)
Epoch  550/1000:Epoch loss: 0.2500 - avg acc: 89.3% - val-roc: 0.6531 - val-ap: 0.6297 (0.1s/epoch)
Epoch  551/1000:Epoch loss: 0.2526 - avg acc: 90.1% - val-roc: 0.6888 - val-ap: 0.6552 (0.1s/epoch)
Epoch  552/1000:Epoch loss: 0.2704 - avg acc: 84.0% - val-roc: 0.6429 - val-ap: 0.6037 (0.1s/epoch)
Epoch  553/1000:Epoch loss: 0.2409 - avg acc: 90.1% - val-roc: 0.6378 - val-ap: 0.6020 (0.1s/epoch)
Epoch  554/1000:Epoch loss: 0.2105 - avg acc: 89.3% - val-roc: 0.6276 - val-ap: 0.5952 (0.1s/epoch)
Epoch  555/1000:Epoch loss: 0.2274 - avg acc: 88.5% - val-roc: 0.6735 - val-ap: 0.6953 (0.1s/epoch)
Epoch  556/1000:Epoch loss: 0.2622 - avg acc: 90.1% - val-roc: 0.7398 - val-ap: 0.7562 (0.1s/epoch)
Epoch  557/1000:Epoch loss: 0.2616 - avg acc: 90.1% - val-roc: 0.7398 - val-ap: 0.7035 (0.1s/epoch)
Epoch  558/1000:Epoch loss: 0.2494 - avg acc: 90.1% - val-roc: 0.7245 - val-ap: 0.6907 (0.1s/epoch)
Epoch  559/1000:Epoch loss: 0.2120 - avg acc: 90.8% - val-roc: 0.7092 - val-ap: 0.6772 (0.1s/epoch)
Epoch  560/1000:Epoch loss: 0.1872 - avg acc: 90.1% - val-roc: 0.6582 - val-ap: 0.6448 (0.1s/epoch)
Epoch  561/1000:Epoch loss: 0.2091 - avg acc: 90.8% - val-roc: 0.6429 - val-ap: 0.6332 (0.1s/epoch)
Epoch  562/1000:Epoch loss: 0.2062 - avg acc: 89.3% - val-roc: 0.6224 - val-ap: 0.6230 (0.1s/epoch)
Epoch  563/1000:Epoch loss: 0.2412 - avg acc: 90.8% - val-roc: 0.6684 - val-ap: 0.6592 (0.1s/epoch)
Epoch  564/1000:Epoch loss: 0.2802 - avg acc: 89.3% - val-roc: 0.6480 - val-ap: 0.6355 (0.1s/epoch)
Epoch  565/1000:Epoch loss: 0.2160 - avg acc: 90.8% - val-roc: 0.6480 - val-ap: 0.6325 (0.1s/epoch)
Epoch  566/1000:Epoch loss: 0.2430 - avg acc: 88.5% - val-roc: 0.6276 - val-ap: 0.6216 (0.1s/epoch)
Epoch  567/1000:Epoch loss: 0.3099 - avg acc: 90.1% - val-roc: 0.6633 - val-ap: 0.6500 (0.1s/epoch)
Epoch  568/1000:Epoch loss: 0.2320 - avg acc: 89.3% - val-roc: 0.7143 - val-ap: 0.7706 (0.1s/epoch)
Epoch  569/1000:Epoch loss: 0.2244 - avg acc: 90.1% - val-roc: 0.6276 - val-ap: 0.6585 (0.1s/epoch)
Epoch  570/1000:Epoch loss: 0.2596 - avg acc: 90.8% - val-roc: 0.6327 - val-ap: 0.6223 (0.1s/epoch)
Epoch  571/1000:Epoch loss: 0.2279 - avg acc: 87.8% - val-roc: 0.5918 - val-ap: 0.6041 (0.1s/epoch)
Epoch  572/1000:Epoch loss: 0.2285 - avg acc: 90.8% - val-roc: 0.6276 - val-ap: 0.6370 (0.1s/epoch)
Epoch  573/1000:Epoch loss: 0.1732 - avg acc: 89.3% - val-roc: 0.6684 - val-ap: 0.7247 (0.1s/epoch)
Epoch  574/1000:Epoch loss: 0.2067 - avg acc: 90.1% - val-roc: 0.6735 - val-ap: 0.7264 (0.1s/epoch)
Epoch  575/1000:Epoch loss: 0.2128 - avg acc: 89.3% - val-roc: 0.6531 - val-ap: 0.6475 (0.1s/epoch)
Epoch  576/1000:Epoch loss: 0.2483 - avg acc: 89.3% - val-roc: 0.6327 - val-ap: 0.6312 (0.1s/epoch)
Epoch  577/1000:Epoch loss: 0.2260 - avg acc: 89.3% - val-roc: 0.6735 - val-ap: 0.6592 (0.1s/epoch)
Epoch  578/1000:Epoch loss: 0.2164 - avg acc: 90.8% - val-roc: 0.6582 - val-ap: 0.6431 (0.1s/epoch)
Epoch  579/1000:Epoch loss: 0.1933 - avg acc: 90.8% - val-roc: 0.6684 - val-ap: 0.6420 (0.1s/epoch)
Epoch  580/1000:Epoch loss: 0.2738 - avg acc: 90.8% - val-roc: 0.6939 - val-ap: 0.6575 (0.1s/epoch)
Epoch  581/1000:Epoch loss: 0.1851 - avg acc: 90.8% - val-roc: 0.6837 - val-ap: 0.6516 (0.1s/epoch)
Epoch  582/1000:Epoch loss: 0.2050 - avg acc: 90.1% - val-roc: 0.6582 - val-ap: 0.6419 (0.1s/epoch)
Epoch  583/1000:Epoch loss: 0.2345 - avg acc: 90.1% - val-roc: 0.6633 - val-ap: 0.6467 (0.1s/epoch)
Epoch  584/1000:Epoch loss: 0.2223 - avg acc: 90.8% - val-roc: 0.6429 - val-ap: 0.6338 (0.1s/epoch)
Epoch  585/1000:Epoch loss: 0.2357 - avg acc: 90.8% - val-roc: 0.6327 - val-ap: 0.6279 (0.1s/epoch)
Epoch  586/1000:Epoch loss: 0.2202 - avg acc: 90.8% - val-roc: 0.7245 - val-ap: 0.7017 (0.1s/epoch)
Epoch  587/1000:Epoch loss: 0.2191 - avg acc: 90.8% - val-roc: 0.7296 - val-ap: 0.7022 (0.1s/epoch)
Epoch  588/1000:Epoch loss: 0.2144 - avg acc: 90.8% - val-roc: 0.6990 - val-ap: 0.6853 (0.1s/epoch)
Epoch  589/1000:Epoch loss: 0.1674 - avg acc: 90.8% - val-roc: 0.6939 - val-ap: 0.6839 (0.1s/epoch)
Epoch  590/1000:Epoch loss: 0.2077 - avg acc: 90.1% - val-roc: 0.6786 - val-ap: 0.6686 (0.1s/epoch)
Epoch  591/1000:Epoch loss: 0.1716 - avg acc: 90.8% - val-roc: 0.6582 - val-ap: 0.6542 (0.1s/epoch)
Epoch  592/1000:Epoch loss: 0.2361 - avg acc: 91.6% - val-roc: 0.6378 - val-ap: 0.6346 (0.1s/epoch)
Epoch  593/1000:Epoch loss: 0.2450 - avg acc: 90.8% - val-roc: 0.6480 - val-ap: 0.6413 (0.1s/epoch)
Epoch  594/1000:Epoch loss: 0.2024 - avg acc: 90.8% - val-roc: 0.6786 - val-ap: 0.6624 (0.1s/epoch)
Epoch  595/1000:Epoch loss: 0.2102 - avg acc: 90.8% - val-roc: 0.6837 - val-ap: 0.6625 (0.1s/epoch)
Epoch  596/1000:Epoch loss: 0.2048 - avg acc: 90.1% - val-roc: 0.7194 - val-ap: 0.7000 (0.1s/epoch)
Epoch  597/1000:Epoch loss: 0.2236 - avg acc: 90.1% - val-roc: 0.6990 - val-ap: 0.6786 (0.1s/epoch)
Epoch  598/1000:Epoch loss: 0.2308 - avg acc: 90.1% - val-roc: 0.7143 - val-ap: 0.6950 (0.1s/epoch)
Epoch  599/1000:Epoch loss: 0.2632 - avg acc: 90.8% - val-roc: 0.6837 - val-ap: 0.6692 (0.1s/epoch)
Epoch  600/1000:Epoch loss: 0.2060 - avg acc: 90.1% - val-roc: 0.6735 - val-ap: 0.6615 (0.1s/epoch)
Epoch  601/1000:Epoch loss: 0.1981 - avg acc: 90.8% - val-roc: 0.7296 - val-ap: 0.6923 (0.1s/epoch)
Epoch  602/1000:Epoch loss: 0.1953 - avg acc: 88.5% - val-roc: 0.7602 - val-ap: 0.7524 (0.1s/epoch)
Epoch  603/1000:Epoch loss: 0.2763 - avg acc: 90.1% - val-roc: 0.7806 - val-ap: 0.7886 (0.1s/epoch)
Epoch  604/1000:Epoch loss: 0.2249 - avg acc: 90.8% - val-roc: 0.7806 - val-ap: 0.8069 (0.1s/epoch)
Epoch  605/1000:Epoch loss: 0.2168 - avg acc: 90.8% - val-roc: 0.7755 - val-ap: 0.8095 (0.1s/epoch)
Epoch  606/1000:Epoch loss: 0.1994 - avg acc: 90.8% - val-roc: 0.7500 - val-ap: 0.7719 (0.1s/epoch)
Epoch  607/1000:Epoch loss: 0.2469 - avg acc: 90.8% - val-roc: 0.6990 - val-ap: 0.7379 (0.1s/epoch)
Epoch  608/1000:Epoch loss: 0.1716 - avg acc: 90.1% - val-roc: 0.7245 - val-ap: 0.7698 (0.1s/epoch)
Epoch  609/1000:Epoch loss: 0.2592 - avg acc: 89.3% - val-roc: 0.7347 - val-ap: 0.7602 (0.1s/epoch)
Epoch  610/1000:Epoch loss: 0.2183 - avg acc: 89.3% - val-roc: 0.7602 - val-ap: 0.7800 (0.1s/epoch)
Epoch  611/1000:Epoch loss: 0.2185 - avg acc: 90.1% - val-roc: 0.7653 - val-ap: 0.7859 (0.2s/epoch)
Epoch  612/1000:Epoch loss: 0.2101 - avg acc: 90.8% - val-roc: 0.6684 - val-ap: 0.6446 (0.1s/epoch)
Epoch  613/1000:Epoch loss: 0.2101 - avg acc: 90.8% - val-roc: 0.6429 - val-ap: 0.6292 (0.1s/epoch)
Epoch  614/1000:Epoch loss: 0.1892 - avg acc: 90.8% - val-roc: 0.6939 - val-ap: 0.6656 (0.1s/epoch)
Epoch  615/1000:Epoch loss: 0.2013 - avg acc: 90.8% - val-roc: 0.7704 - val-ap: 0.7401 (0.1s/epoch)
Epoch  616/1000:Epoch loss: 0.2190 - avg acc: 90.1% - val-roc: 0.7806 - val-ap: 0.7778 (0.1s/epoch)
Epoch  617/1000:Epoch loss: 0.2089 - avg acc: 90.1% - val-roc: 0.7806 - val-ap: 0.7766 (0.1s/epoch)
Epoch  618/1000:Epoch loss: 0.2020 - avg acc: 91.6% - val-roc: 0.7602 - val-ap: 0.7241 (0.1s/epoch)
Epoch  619/1000:Epoch loss: 0.2280 - avg acc: 90.8% - val-roc: 0.7857 - val-ap: 0.7253 (0.1s/epoch)
Epoch  620/1000:Epoch loss: 0.1914 - avg acc: 90.8% - val-roc: 0.8316 - val-ap: 0.8008 (0.1s/epoch)
Epoch  621/1000:Epoch loss: 0.2079 - avg acc: 90.8% - val-roc: 0.8163 - val-ap: 0.8300 (0.1s/epoch)
Epoch  622/1000:Epoch loss: 0.1721 - avg acc: 91.6% - val-roc: 0.8265 - val-ap: 0.8352 (0.1s/epoch)
Epoch  623/1000:Epoch loss: 0.1921 - avg acc: 74.8% - val-roc: 0.8010 - val-ap: 0.8160 (0.1s/epoch)
Epoch  624/1000:Epoch loss: 0.1982 - avg acc: 90.8% - val-roc: 0.8265 - val-ap: 0.8472 (0.1s/epoch)
Epoch  625/1000:Epoch loss: 0.2255 - avg acc: 90.1% - val-roc: 0.7857 - val-ap: 0.7898 (0.1s/epoch)
Epoch  626/1000:Epoch loss: 0.2988 - avg acc: 90.8% - val-roc: 0.7551 - val-ap: 0.7453 (0.1s/epoch)
Epoch  627/1000:Epoch loss: 0.1856 - avg acc: 91.6% - val-roc: 0.7194 - val-ap: 0.6659 (0.1s/epoch)
Epoch  628/1000:Epoch loss: 0.1984 - avg acc: 91.6% - val-roc: 0.7449 - val-ap: 0.6887 (0.1s/epoch)
Epoch  629/1000:Epoch loss: 0.2019 - avg acc: 91.6% - val-roc: 0.7449 - val-ap: 0.6937 (0.1s/epoch)
Epoch  630/1000:Epoch loss: 0.2196 - avg acc: 90.1% - val-roc: 0.7398 - val-ap: 0.6931 (0.1s/epoch)
Epoch  631/1000:Epoch loss: 0.2301 - avg acc: 89.3% - val-roc: 0.7347 - val-ap: 0.6925 (0.1s/epoch)
Epoch  632/1000:Epoch loss: 0.1962 - avg acc: 90.8% - val-roc: 0.7398 - val-ap: 0.6925 (0.1s/epoch)
Epoch  633/1000:Epoch loss: 0.2144 - avg acc: 84.0% - val-roc: 0.6990 - val-ap: 0.6519 (0.1s/epoch)
Epoch  634/1000:Epoch loss: 0.2135 - avg acc: 90.8% - val-roc: 0.7704 - val-ap: 0.7034 (0.2s/epoch)
Epoch  635/1000:Epoch loss: 0.2226 - avg acc: 90.1% - val-roc: 0.7755 - val-ap: 0.7566 (0.1s/epoch)
Epoch  636/1000:Epoch loss: 0.2085 - avg acc: 90.8% - val-roc: 0.7653 - val-ap: 0.7139 (0.1s/epoch)
Epoch  637/1000:Epoch loss: 0.1853 - avg acc: 84.0% - val-roc: 0.7500 - val-ap: 0.7029 (0.1s/epoch)
Epoch  638/1000:Epoch loss: 0.2213 - avg acc: 90.8% - val-roc: 0.7347 - val-ap: 0.6907 (0.1s/epoch)
Epoch  639/1000:Epoch loss: 0.2022 - avg acc: 91.6% - val-roc: 0.7245 - val-ap: 0.6964 (0.1s/epoch)
Epoch  640/1000:Epoch loss: 0.2124 - avg acc: 90.8% - val-roc: 0.7143 - val-ap: 0.6778 (0.1s/epoch)
Epoch  641/1000:Epoch loss: 0.1792 - avg acc: 90.8% - val-roc: 0.7602 - val-ap: 0.7354 (0.1s/epoch)
Epoch  642/1000:Epoch loss: 0.2076 - avg acc: 90.8% - val-roc: 0.7857 - val-ap: 0.7516 (0.1s/epoch)
Epoch  643/1000:Epoch loss: 0.2037 - avg acc: 84.0% - val-roc: 0.7602 - val-ap: 0.7042 (0.1s/epoch)
Epoch  644/1000:Epoch loss: 0.1709 - avg acc: 90.8% - val-roc: 0.7908 - val-ap: 0.7713 (0.1s/epoch)
Epoch  645/1000:Epoch loss: 0.2696 - avg acc: 90.1% - val-roc: 0.8010 - val-ap: 0.8055 (0.1s/epoch)
Epoch  646/1000:Epoch loss: 0.2187 - avg acc: 90.8% - val-roc: 0.7602 - val-ap: 0.7015 (0.1s/epoch)
Epoch  647/1000:Epoch loss: 0.1938 - avg acc: 91.6% - val-roc: 0.7551 - val-ap: 0.6799 (0.1s/epoch)
Epoch  648/1000:Epoch loss: 0.2293 - avg acc: 90.8% - val-roc: 0.7449 - val-ap: 0.6785 (0.1s/epoch)
Epoch  649/1000:Epoch loss: 0.2204 - avg acc: 91.6% - val-roc: 0.7755 - val-ap: 0.6972 (0.1s/epoch)
Epoch  650/1000:Epoch loss: 0.2449 - avg acc: 83.2% - val-roc: 0.7551 - val-ap: 0.6940 (0.1s/epoch)
Epoch  651/1000:Epoch loss: 0.2306 - avg acc: 92.4% - val-roc: 0.7806 - val-ap: 0.7183 (0.1s/epoch)
Epoch  652/1000:Epoch loss: 0.1892 - avg acc: 83.2% - val-roc: 0.8010 - val-ap: 0.8080 (0.1s/epoch)
Epoch  653/1000:Epoch loss: 0.2012 - avg acc: 90.8% - val-roc: 0.8418 - val-ap: 0.8622 (0.1s/epoch)
Epoch  654/1000:Epoch loss: 0.2668 - avg acc: 90.1% - val-roc: 0.8878 - val-ap: 0.9107 (0.1s/epoch)
Epoch  655/1000:Epoch loss: 0.2834 - avg acc: 89.3% - val-roc: 0.8265 - val-ap: 0.8167 (0.1s/epoch)
Epoch  656/1000:Epoch loss: 0.2635 - avg acc: 93.1% - val-roc: 0.7959 - val-ap: 0.7589 (0.1s/epoch)
Epoch  657/1000:Epoch loss: 0.2869 - avg acc: 87.8% - val-roc: 0.8265 - val-ap: 0.8316 (0.1s/epoch)
Epoch  658/1000:Epoch loss: 0.2152 - avg acc: 91.6% - val-roc: 0.8367 - val-ap: 0.8472 (0.1s/epoch)
Epoch  659/1000:Epoch loss: 0.1602 - avg acc: 88.5% - val-roc: 0.8061 - val-ap: 0.8172 (0.1s/epoch)
Epoch  660/1000:Epoch loss: 0.1683 - avg acc: 88.5% - val-roc: 0.7959 - val-ap: 0.7995 (0.1s/epoch)
Epoch  661/1000:Epoch loss: 0.1832 - avg acc: 88.5% - val-roc: 0.7959 - val-ap: 0.8002 (0.1s/epoch)
Epoch  662/1000:Epoch loss: 0.1687 - avg acc: 88.5% - val-roc: 0.7755 - val-ap: 0.7904 (0.1s/epoch)
Epoch  663/1000:Epoch loss: 0.2049 - avg acc: 89.3% - val-roc: 0.7806 - val-ap: 0.7647 (0.1s/epoch)
Epoch  664/1000:Epoch loss: 0.2085 - avg acc: 82.4% - val-roc: 0.7959 - val-ap: 0.7608 (0.1s/epoch)
Epoch  665/1000:Epoch loss: 0.2146 - avg acc: 89.3% - val-roc: 0.8520 - val-ap: 0.8439 (0.1s/epoch)
Epoch  666/1000:Epoch loss: 0.1821 - avg acc: 87.0% - val-roc: 0.8316 - val-ap: 0.8342 (0.1s/epoch)
Epoch  667/1000:Epoch loss: 0.2236 - avg acc: 93.1% - val-roc: 0.7857 - val-ap: 0.7606 (0.1s/epoch)
Epoch  668/1000:Epoch loss: 0.2359 - avg acc: 82.4% - val-roc: 0.7551 - val-ap: 0.7414 (0.1s/epoch)
Epoch  669/1000:Epoch loss: 0.1765 - avg acc: 93.9% - val-roc: 0.8163 - val-ap: 0.8046 (0.1s/epoch)
Epoch  670/1000:Epoch loss: 0.1871 - avg acc: 90.8% - val-roc: 0.7704 - val-ap: 0.7057 (0.1s/epoch)
Epoch  671/1000:Epoch loss: 0.1560 - avg acc: 91.6% - val-roc: 0.7806 - val-ap: 0.7155 (0.1s/epoch)
Epoch  672/1000:Epoch loss: 0.1855 - avg acc: 88.5% - val-roc: 0.7653 - val-ap: 0.7013 (0.1s/epoch)
Epoch  673/1000:Epoch loss: 0.1557 - avg acc: 90.8% - val-roc: 0.7653 - val-ap: 0.6996 (0.1s/epoch)
Epoch  674/1000:Epoch loss: 0.2089 - avg acc: 90.1% - val-roc: 0.7296 - val-ap: 0.6721 (0.1s/epoch)
Epoch  675/1000:Epoch loss: 0.1816 - avg acc: 90.8% - val-roc: 0.6837 - val-ap: 0.6551 (0.1s/epoch)
Epoch  676/1000:Epoch loss: 0.1537 - avg acc: 93.1% - val-roc: 0.6837 - val-ap: 0.6546 (0.1s/epoch)
Epoch  677/1000:Epoch loss: 0.1417 - avg acc: 91.6% - val-roc: 0.6990 - val-ap: 0.6685 (0.1s/epoch)
Epoch  678/1000:Epoch loss: 0.1823 - avg acc: 92.4% - val-roc: 0.7449 - val-ap: 0.7053 (0.1s/epoch)
Epoch  679/1000:Epoch loss: 0.1515 - avg acc: 93.9% - val-roc: 0.7194 - val-ap: 0.7107 (0.1s/epoch)
Epoch  680/1000:Epoch loss: 0.2435 - avg acc: 84.0% - val-roc: 0.6990 - val-ap: 0.6958 (0.1s/epoch)
Epoch  681/1000:Epoch loss: 0.2240 - avg acc: 91.6% - val-roc: 0.7679 - val-ap: 0.7428 (0.1s/epoch)
Epoch  682/1000:Epoch loss: 0.2493 - avg acc: 82.4% - val-roc: 0.7449 - val-ap: 0.6914 (0.1s/epoch)
Epoch  683/1000:Epoch loss: 0.2110 - avg acc: 93.1% - val-roc: 0.7500 - val-ap: 0.6880 (0.1s/epoch)
Epoch  684/1000:Epoch loss: 0.1751 - avg acc: 90.8% - val-roc: 0.7245 - val-ap: 0.6543 (0.1s/epoch)
Epoch  685/1000:Epoch loss: 0.1872 - avg acc: 90.8% - val-roc: 0.7092 - val-ap: 0.6464 (0.1s/epoch)
Epoch  686/1000:Epoch loss: 0.1578 - avg acc: 90.8% - val-roc: 0.7551 - val-ap: 0.6772 (0.1s/epoch)
Epoch  687/1000:Epoch loss: 0.1906 - avg acc: 92.4% - val-roc: 0.7704 - val-ap: 0.7052 (0.1s/epoch)
Epoch  688/1000:Epoch loss: 0.2028 - avg acc: 91.6% - val-roc: 0.7653 - val-ap: 0.7010 (0.1s/epoch)
Epoch  689/1000:Epoch loss: 0.1869 - avg acc: 90.8% - val-roc: 0.7296 - val-ap: 0.6665 (0.1s/epoch)
Epoch  690/1000:Epoch loss: 0.2368 - avg acc: 93.1% - val-roc: 0.7347 - val-ap: 0.6680 (0.1s/epoch)
Epoch  691/1000:Epoch loss: 0.1954 - avg acc: 91.6% - val-roc: 0.7143 - val-ap: 0.6601 (0.1s/epoch)
Epoch  692/1000:Epoch loss: 0.2368 - avg acc: 90.1% - val-roc: 0.6990 - val-ap: 0.6622 (0.1s/epoch)
Epoch  693/1000:Epoch loss: 0.1912 - avg acc: 93.1% - val-roc: 0.7602 - val-ap: 0.7096 (0.1s/epoch)
Epoch  694/1000:Epoch loss: 0.2085 - avg acc: 93.1% - val-roc: 0.7653 - val-ap: 0.7149 (0.1s/epoch)
Epoch  695/1000:Epoch loss: 0.1788 - avg acc: 92.4% - val-roc: 0.7551 - val-ap: 0.7011 (0.1s/epoch)
Epoch  696/1000:Epoch loss: 0.1415 - avg acc: 91.6% - val-roc: 0.7296 - val-ap: 0.6806 (0.1s/epoch)
Epoch  697/1000:Epoch loss: 0.1828 - avg acc: 93.9% - val-roc: 0.7500 - val-ap: 0.7004 (0.1s/epoch)
Epoch  698/1000:Epoch loss: 0.1799 - avg acc: 91.6% - val-roc: 0.6939 - val-ap: 0.6670 (0.1s/epoch)
Epoch  699/1000:Epoch loss: 0.1280 - avg acc: 93.1% - val-roc: 0.6939 - val-ap: 0.6674 (0.1s/epoch)
Epoch  700/1000:Epoch loss: 0.1554 - avg acc: 89.3% - val-roc: 0.6633 - val-ap: 0.6436 (0.1s/epoch)
Epoch  701/1000:Epoch loss: 0.1601 - avg acc: 93.1% - val-roc: 0.6939 - val-ap: 0.6734 (0.1s/epoch)
Epoch  702/1000:Epoch loss: 0.1552 - avg acc: 90.8% - val-roc: 0.7602 - val-ap: 0.7202 (0.1s/epoch)
Epoch  703/1000:Epoch loss: 0.2019 - avg acc: 93.9% - val-roc: 0.7602 - val-ap: 0.6985 (0.1s/epoch)
Epoch  704/1000:Epoch loss: 0.2289 - avg acc: 84.7% - val-roc: 0.6786 - val-ap: 0.6410 (0.1s/epoch)
Epoch  705/1000:Epoch loss: 0.1906 - avg acc: 91.6% - val-roc: 0.6990 - val-ap: 0.6677 (0.1s/epoch)
Epoch  706/1000:Epoch loss: 0.1746 - avg acc: 92.4% - val-roc: 0.7704 - val-ap: 0.7560 (0.1s/epoch)
Epoch  707/1000:Epoch loss: 0.1644 - avg acc: 90.8% - val-roc: 0.7857 - val-ap: 0.7347 (0.1s/epoch)
Epoch  708/1000:Epoch loss: 0.1694 - avg acc: 89.3% - val-roc: 0.7296 - val-ap: 0.6757 (0.1s/epoch)
Epoch  709/1000:Epoch loss: 0.1328 - avg acc: 90.8% - val-roc: 0.7245 - val-ap: 0.6694 (0.1s/epoch)
Epoch  710/1000:Epoch loss: 0.1808 - avg acc: 90.1% - val-roc: 0.7041 - val-ap: 0.6562 (0.1s/epoch)
Epoch  711/1000:Epoch loss: 0.2091 - avg acc: 90.8% - val-roc: 0.7347 - val-ap: 0.6793 (0.1s/epoch)
Epoch  712/1000:Epoch loss: 0.2119 - avg acc: 88.5% - val-roc: 0.7194 - val-ap: 0.6667 (0.1s/epoch)
Epoch  713/1000:Epoch loss: 0.1659 - avg acc: 84.7% - val-roc: 0.7347 - val-ap: 0.6803 (0.1s/epoch)
Epoch  714/1000:Epoch loss: 0.1974 - avg acc: 83.2% - val-roc: 0.7500 - val-ap: 0.7031 (0.1s/epoch)
Epoch  715/1000:Epoch loss: 0.1724 - avg acc: 86.3% - val-roc: 0.8112 - val-ap: 0.8063 (0.1s/epoch)
Epoch  716/1000:Epoch loss: 0.1511 - avg acc: 91.6% - val-roc: 0.8367 - val-ap: 0.8216 (0.1s/epoch)
Epoch  717/1000:Epoch loss: 0.1860 - avg acc: 88.5% - val-roc: 0.8316 - val-ap: 0.8198 (0.1s/epoch)
Epoch  718/1000:Epoch loss: 0.2078 - avg acc: 89.3% - val-roc: 0.8367 - val-ap: 0.8381 (0.1s/epoch)
Epoch  719/1000:Epoch loss: 0.1555 - avg acc: 80.2% - val-roc: 0.8010 - val-ap: 0.7899 (0.1s/epoch)
Epoch  720/1000:Epoch loss: 0.2090 - avg acc: 91.6% - val-roc: 0.8418 - val-ap: 0.8003 (0.1s/epoch)
Epoch  721/1000:Epoch loss: 0.1781 - avg acc: 88.5% - val-roc: 0.8571 - val-ap: 0.8454 (0.1s/epoch)
Epoch  722/1000:Epoch loss: 0.1576 - avg acc: 90.8% - val-roc: 0.8520 - val-ap: 0.8396 (0.1s/epoch)
Epoch  723/1000:Epoch loss: 0.1710 - avg acc: 94.7% - val-roc: 0.8061 - val-ap: 0.7530 (0.1s/epoch)
Epoch  724/1000:Epoch loss: 0.2096 - avg acc: 81.7% - val-roc: 0.8163 - val-ap: 0.7830 (0.1s/epoch)
Epoch  725/1000:Epoch loss: 0.1828 - avg acc: 94.7% - val-roc: 0.8214 - val-ap: 0.7939 (0.1s/epoch)
Epoch  726/1000:Epoch loss: 0.1724 - avg acc: 92.4% - val-roc: 0.8163 - val-ap: 0.8007 (0.1s/epoch)
Epoch  727/1000:Epoch loss: 0.1767 - avg acc: 80.9% - val-roc: 0.7857 - val-ap: 0.7753 (0.1s/epoch)
Epoch  728/1000:Epoch loss: 0.2085 - avg acc: 93.9% - val-roc: 0.7857 - val-ap: 0.7106 (0.1s/epoch)
Epoch  729/1000:Epoch loss: 0.1575 - avg acc: 90.1% - val-roc: 0.7500 - val-ap: 0.6865 (0.1s/epoch)
Epoch  730/1000:Epoch loss: 0.1989 - avg acc: 90.8% - val-roc: 0.7704 - val-ap: 0.7038 (0.1s/epoch)
Epoch  731/1000:Epoch loss: 0.1855 - avg acc: 93.9% - val-roc: 0.7755 - val-ap: 0.7013 (0.1s/epoch)
Epoch  732/1000:Epoch loss: 0.1783 - avg acc: 84.7% - val-roc: 0.7398 - val-ap: 0.7348 (0.1s/epoch)
Epoch  733/1000:Epoch loss: 0.1578 - avg acc: 90.8% - val-roc: 0.7653 - val-ap: 0.7730 (0.1s/epoch)
Epoch  734/1000:Epoch loss: 0.1279 - avg acc: 89.3% - val-roc: 0.7551 - val-ap: 0.7735 (0.1s/epoch)
Epoch  735/1000:Epoch loss: 0.1386 - avg acc: 90.1% - val-roc: 0.8010 - val-ap: 0.8186 (0.1s/epoch)
Epoch  736/1000:Epoch loss: 0.1711 - avg acc: 90.8% - val-roc: 0.7755 - val-ap: 0.7443 (0.1s/epoch)
Epoch  737/1000:Epoch loss: 0.2016 - avg acc: 91.6% - val-roc: 0.7551 - val-ap: 0.6841 (0.1s/epoch)
Epoch  738/1000:Epoch loss: 0.1732 - avg acc: 91.6% - val-roc: 0.8061 - val-ap: 0.8093 (0.1s/epoch)
Epoch  739/1000:Epoch loss: 0.1455 - avg acc: 91.6% - val-roc: 0.8367 - val-ap: 0.8603 (0.1s/epoch)
Epoch  740/1000:Epoch loss: 0.1503 - avg acc: 89.3% - val-roc: 0.8469 - val-ap: 0.8857 (0.1s/epoch)
Epoch  741/1000:Epoch loss: 0.2351 - avg acc: 88.5% - val-roc: 0.8571 - val-ap: 0.8938 (0.1s/epoch)
Epoch  742/1000:Epoch loss: 0.2093 - avg acc: 90.1% - val-roc: 0.7959 - val-ap: 0.7904 (0.1s/epoch)
Epoch  743/1000:Epoch loss: 0.1752 - avg acc: 91.6% - val-roc: 0.7602 - val-ap: 0.7292 (0.1s/epoch)
Epoch  744/1000:Epoch loss: 0.2069 - avg acc: 83.2% - val-roc: 0.7500 - val-ap: 0.7399 (0.1s/epoch)
Epoch  745/1000:Epoch loss: 0.1835 - avg acc: 90.1% - val-roc: 0.8061 - val-ap: 0.8338 (0.1s/epoch)
Epoch  746/1000:Epoch loss: 0.2143 - avg acc: 90.8% - val-roc: 0.7857 - val-ap: 0.7726 (0.1s/epoch)
Epoch  747/1000:Epoch loss: 0.1644 - avg acc: 88.5% - val-roc: 0.7806 - val-ap: 0.7497 (0.1s/epoch)
Epoch  748/1000:Epoch loss: 0.2204 - avg acc: 93.1% - val-roc: 0.7959 - val-ap: 0.7832 (0.1s/epoch)
Epoch  749/1000:Epoch loss: 0.2251 - avg acc: 90.8% - val-roc: 0.7704 - val-ap: 0.7829 (0.1s/epoch)
Epoch  750/1000:Epoch loss: 0.2318 - avg acc: 90.1% - val-roc: 0.8061 - val-ap: 0.8206 (0.1s/epoch)
Epoch  751/1000:Epoch loss: 0.2062 - avg acc: 93.9% - val-roc: 0.7653 - val-ap: 0.7212 (0.1s/epoch)
Epoch  752/1000:Epoch loss: 0.2279 - avg acc: 64.1% - val-roc: 0.6888 - val-ap: 0.7111 (0.1s/epoch)
Epoch  753/1000:Epoch loss: 0.2200 - avg acc: 91.6% - val-roc: 0.7806 - val-ap: 0.8097 (0.1s/epoch)
Epoch  754/1000:Epoch loss: 0.2151 - avg acc: 90.1% - val-roc: 0.7296 - val-ap: 0.7450 (0.1s/epoch)
Epoch  755/1000:Epoch loss: 0.1156 - avg acc: 95.4% - val-roc: 0.7296 - val-ap: 0.6909 (0.1s/epoch)
Epoch  756/1000:Epoch loss: 0.1872 - avg acc: 89.3% - val-roc: 0.7704 - val-ap: 0.7836 (0.1s/epoch)
Epoch  757/1000:Epoch loss: 0.1772 - avg acc: 96.2% - val-roc: 0.7449 - val-ap: 0.7630 (0.1s/epoch)
Epoch  758/1000:Epoch loss: 0.1566 - avg acc: 92.4% - val-roc: 0.7143 - val-ap: 0.7202 (0.1s/epoch)
Epoch  759/1000:Epoch loss: 0.2269 - avg acc: 81.7% - val-roc: 0.7143 - val-ap: 0.7175 (0.1s/epoch)
Epoch  760/1000:Epoch loss: 0.1660 - avg acc: 92.4% - val-roc: 0.7755 - val-ap: 0.7732 (0.1s/epoch)
Epoch  761/1000:Epoch loss: 0.1846 - avg acc: 93.1% - val-roc: 0.7092 - val-ap: 0.7268 (0.1s/epoch)
Epoch  762/1000:Epoch loss: 0.1507 - avg acc: 90.8% - val-roc: 0.7194 - val-ap: 0.7358 (0.1s/epoch)
Epoch  763/1000:Epoch loss: 0.1912 - avg acc: 93.1% - val-roc: 0.7092 - val-ap: 0.6782 (0.1s/epoch)
Epoch  764/1000:Epoch loss: 0.1609 - avg acc: 90.8% - val-roc: 0.7041 - val-ap: 0.6703 (0.1s/epoch)
Epoch  765/1000:Epoch loss: 0.1490 - avg acc: 94.7% - val-roc: 0.7602 - val-ap: 0.7167 (0.1s/epoch)
Epoch  766/1000:Epoch loss: 0.2025 - avg acc: 94.7% - val-roc: 0.7908 - val-ap: 0.8061 (0.1s/epoch)
Epoch  767/1000:Epoch loss: 0.1557 - avg acc: 94.7% - val-roc: 0.7500 - val-ap: 0.7012 (0.1s/epoch)
Epoch  768/1000:Epoch loss: 0.1714 - avg acc: 93.1% - val-roc: 0.7398 - val-ap: 0.6891 (0.1s/epoch)
Epoch  769/1000:Epoch loss: 0.1585 - avg acc: 93.9% - val-roc: 0.7857 - val-ap: 0.7710 (0.1s/epoch)
Epoch  770/1000:Epoch loss: 0.1447 - avg acc: 92.4% - val-roc: 0.8112 - val-ap: 0.8284 (0.1s/epoch)
Epoch  771/1000:Epoch loss: 0.1522 - avg acc: 92.4% - val-roc: 0.8163 - val-ap: 0.8306 (0.1s/epoch)
Epoch  772/1000:Epoch loss: 0.1506 - avg acc: 93.9% - val-roc: 0.8316 - val-ap: 0.8520 (0.1s/epoch)
Epoch  773/1000:Epoch loss: 0.1569 - avg acc: 90.8% - val-roc: 0.8673 - val-ap: 0.9088 (0.1s/epoch)
Epoch  774/1000:Epoch loss: 0.1752 - avg acc: 93.1% - val-roc: 0.8827 - val-ap: 0.9191 (0.1s/epoch)
Epoch  775/1000:Epoch loss: 0.1604 - avg acc: 92.4% - val-roc: 0.8316 - val-ap: 0.8610 (0.1s/epoch)
Epoch  776/1000:Epoch loss: 0.2128 - avg acc: 93.1% - val-roc: 0.8061 - val-ap: 0.8153 (0.1s/epoch)
Epoch  777/1000:Epoch loss: 0.1764 - avg acc: 93.1% - val-roc: 0.8163 - val-ap: 0.8345 (0.1s/epoch)
Epoch  778/1000:Epoch loss: 0.1568 - avg acc: 92.4% - val-roc: 0.8061 - val-ap: 0.8281 (0.1s/epoch)
Epoch  779/1000:Epoch loss: 0.1815 - avg acc: 93.1% - val-roc: 0.7908 - val-ap: 0.8184 (0.1s/epoch)
Epoch  780/1000:Epoch loss: 0.1887 - avg acc: 93.9% - val-roc: 0.7806 - val-ap: 0.8150 (0.1s/epoch)
Epoch  781/1000:Epoch loss: 0.1610 - avg acc: 91.6% - val-roc: 0.7347 - val-ap: 0.7608 (0.1s/epoch)
Epoch  782/1000:Epoch loss: 0.2333 - avg acc: 93.9% - val-roc: 0.7755 - val-ap: 0.7893 (0.1s/epoch)
Epoch  783/1000:Epoch loss: 0.1274 - avg acc: 90.1% - val-roc: 0.7704 - val-ap: 0.7906 (0.1s/epoch)
Epoch  784/1000:Epoch loss: 0.1611 - avg acc: 93.1% - val-roc: 0.7653 - val-ap: 0.7730 (0.1s/epoch)
Epoch  785/1000:Epoch loss: 0.1782 - avg acc: 89.3% - val-roc: 0.7449 - val-ap: 0.7372 (0.1s/epoch)
Epoch  786/1000:Epoch loss: 0.1512 - avg acc: 93.1% - val-roc: 0.7704 - val-ap: 0.7340 (0.1s/epoch)
Epoch  787/1000:Epoch loss: 0.2299 - avg acc: 91.6% - val-roc: 0.7755 - val-ap: 0.7362 (0.1s/epoch)
Epoch  788/1000:Epoch loss: 0.1890 - avg acc: 88.5% - val-roc: 0.7704 - val-ap: 0.7334 (0.1s/epoch)
Epoch  789/1000:Epoch loss: 0.1497 - avg acc: 92.4% - val-roc: 0.7704 - val-ap: 0.7363 (0.1s/epoch)
Epoch  790/1000:Epoch loss: 0.2157 - avg acc: 92.4% - val-roc: 0.7959 - val-ap: 0.7642 (0.1s/epoch)
Epoch  791/1000:Epoch loss: 0.1275 - avg acc: 91.6% - val-roc: 0.8316 - val-ap: 0.8021 (0.1s/epoch)
Epoch  792/1000:Epoch loss: 0.1619 - avg acc: 94.7% - val-roc: 0.8163 - val-ap: 0.7805 (0.1s/epoch)
Epoch  793/1000:Epoch loss: 0.1611 - avg acc: 93.1% - val-roc: 0.8163 - val-ap: 0.7841 (0.1s/epoch)
Epoch  794/1000:Epoch loss: 0.1157 - avg acc: 91.6% - val-roc: 0.8418 - val-ap: 0.8060 (0.1s/epoch)
Epoch  795/1000:Epoch loss: 0.1707 - avg acc: 93.9% - val-roc: 0.8673 - val-ap: 0.8453 (0.1s/epoch)
Epoch  796/1000:Epoch loss: 0.1967 - avg acc: 90.1% - val-roc: 0.8673 - val-ap: 0.8689 (0.1s/epoch)
Epoch  797/1000:Epoch loss: 0.1869 - avg acc: 93.9% - val-roc: 0.8265 - val-ap: 0.8189 (0.1s/epoch)
Epoch  798/1000:Epoch loss: 0.1980 - avg acc: 91.6% - val-roc: 0.8010 - val-ap: 0.8037 (0.1s/epoch)
Epoch  799/1000:Epoch loss: 0.1428 - avg acc: 93.9% - val-roc: 0.8418 - val-ap: 0.8429 (0.1s/epoch)
Epoch  800/1000:Epoch loss: 0.1186 - avg acc: 89.3% - val-roc: 0.8469 - val-ap: 0.8565 (0.1s/epoch)
Epoch  801/1000:Epoch loss: 0.1536 - avg acc: 94.7% - val-roc: 0.8469 - val-ap: 0.8463 (0.1s/epoch)
Epoch  802/1000:Epoch loss: 0.1772 - avg acc: 93.1% - val-roc: 0.8112 - val-ap: 0.8120 (0.1s/epoch)
Epoch  803/1000:Epoch loss: 0.1612 - avg acc: 89.3% - val-roc: 0.8061 - val-ap: 0.8059 (0.1s/epoch)
Epoch  804/1000:Epoch loss: 0.1598 - avg acc: 91.6% - val-roc: 0.8112 - val-ap: 0.8088 (0.1s/epoch)
Epoch  805/1000:Epoch loss: 0.1328 - avg acc: 93.1% - val-roc: 0.8061 - val-ap: 0.8066 (0.1s/epoch)
Epoch  806/1000:Epoch loss: 0.1515 - avg acc: 92.4% - val-roc: 0.8316 - val-ap: 0.8234 (0.1s/epoch)
Epoch  807/1000:Epoch loss: 0.1201 - avg acc: 93.9% - val-roc: 0.8214 - val-ap: 0.7945 (0.1s/epoch)
Epoch  808/1000:Epoch loss: 0.1708 - avg acc: 90.8% - val-roc: 0.7857 - val-ap: 0.7239 (0.1s/epoch)
Epoch  809/1000:Epoch loss: 0.1798 - avg acc: 93.9% - val-roc: 0.7551 - val-ap: 0.6909 (0.1s/epoch)
Epoch  810/1000:Epoch loss: 0.1528 - avg acc: 84.7% - val-roc: 0.7500 - val-ap: 0.7259 (0.1s/epoch)
Epoch  811/1000:Epoch loss: 0.1675 - avg acc: 93.1% - val-roc: 0.8163 - val-ap: 0.8161 (0.1s/epoch)
Epoch  812/1000:Epoch loss: 0.1704 - avg acc: 93.1% - val-roc: 0.8010 - val-ap: 0.7505 (0.1s/epoch)
Epoch  813/1000:Epoch loss: 0.1344 - avg acc: 92.4% - val-roc: 0.7449 - val-ap: 0.6995 (0.1s/epoch)
Epoch  814/1000:Epoch loss: 0.1641 - avg acc: 96.2% - val-roc: 0.7602 - val-ap: 0.7067 (0.1s/epoch)
Epoch  815/1000:Epoch loss: 0.1840 - avg acc: 91.6% - val-roc: 0.7653 - val-ap: 0.7469 (0.1s/epoch)
Epoch  816/1000:Epoch loss: 0.1331 - avg acc: 93.1% - val-roc: 0.8929 - val-ap: 0.9229 (0.1s/epoch)
Epoch  817/1000:Epoch loss: 0.1447 - avg acc: 91.6% - val-roc: 0.8980 - val-ap: 0.9280 (0.1s/epoch)
Epoch  818/1000:Epoch loss: 0.1555 - avg acc: 89.3% - val-roc: 0.8673 - val-ap: 0.9004 (0.1s/epoch)
Epoch  819/1000:Epoch loss: 0.1503 - avg acc: 94.7% - val-roc: 0.8112 - val-ap: 0.8353 (0.1s/epoch)
Epoch  820/1000:Epoch loss: 0.1352 - avg acc: 95.4% - val-roc: 0.8010 - val-ap: 0.8182 (0.1s/epoch)
Epoch  821/1000:Epoch loss: 0.2089 - avg acc: 91.6% - val-roc: 0.8214 - val-ap: 0.8467 (0.1s/epoch)
Epoch  822/1000:Epoch loss: 0.2076 - avg acc: 93.1% - val-roc: 0.8520 - val-ap: 0.8631 (0.1s/epoch)
Epoch  823/1000:Epoch loss: 0.1552 - avg acc: 93.1% - val-roc: 0.8469 - val-ap: 0.8308 (0.1s/epoch)
Epoch  824/1000:Epoch loss: 0.1417 - avg acc: 91.6% - val-roc: 0.7959 - val-ap: 0.7774 (0.1s/epoch)
Epoch  825/1000:Epoch loss: 0.1257 - avg acc: 93.9% - val-roc: 0.7653 - val-ap: 0.7476 (0.1s/epoch)
Epoch  826/1000:Epoch loss: 0.1566 - avg acc: 93.9% - val-roc: 0.7806 - val-ap: 0.7543 (0.1s/epoch)
Epoch  827/1000:Epoch loss: 0.1729 - avg acc: 93.9% - val-roc: 0.7959 - val-ap: 0.7680 (0.1s/epoch)
Epoch  828/1000:Epoch loss: 0.1477 - avg acc: 88.5% - val-roc: 0.7704 - val-ap: 0.7825 (0.1s/epoch)
Epoch  829/1000:Epoch loss: 0.1493 - avg acc: 90.1% - val-roc: 0.8061 - val-ap: 0.8226 (0.1s/epoch)
Epoch  830/1000:Epoch loss: 0.1572 - avg acc: 90.8% - val-roc: 0.7653 - val-ap: 0.7655 (0.1s/epoch)
Epoch  831/1000:Epoch loss: 0.1363 - avg acc: 91.6% - val-roc: 0.7449 - val-ap: 0.7229 (0.1s/epoch)
Epoch  832/1000:Epoch loss: 0.1469 - avg acc: 91.6% - val-roc: 0.7551 - val-ap: 0.7201 (0.1s/epoch)
Epoch  833/1000:Epoch loss: 0.1576 - avg acc: 90.8% - val-roc: 0.8112 - val-ap: 0.7732 (0.1s/epoch)
Epoch  834/1000:Epoch loss: 0.2267 - avg acc: 93.9% - val-roc: 0.8367 - val-ap: 0.8191 (0.1s/epoch)
Epoch  835/1000:Epoch loss: 0.1720 - avg acc: 82.4% - val-roc: 0.8214 - val-ap: 0.8311 (0.1s/epoch)
Epoch  836/1000:Epoch loss: 0.1489 - avg acc: 94.7% - val-roc: 0.8214 - val-ap: 0.8364 (0.1s/epoch)
Epoch  837/1000:Epoch loss: 0.1268 - avg acc: 91.6% - val-roc: 0.8010 - val-ap: 0.8206 (0.1s/epoch)
Epoch  838/1000:Epoch loss: 0.1926 - avg acc: 91.6% - val-roc: 0.7755 - val-ap: 0.7852 (0.1s/epoch)
Epoch  839/1000:Epoch loss: 0.1540 - avg acc: 93.9% - val-roc: 0.8163 - val-ap: 0.7996 (0.1s/epoch)
Epoch  840/1000:Epoch loss: 0.1560 - avg acc: 84.0% - val-roc: 0.7806 - val-ap: 0.7789 (0.1s/epoch)
Epoch  841/1000:Epoch loss: 0.2349 - avg acc: 92.4% - val-roc: 0.7653 - val-ap: 0.7654 (0.1s/epoch)
Epoch  842/1000:Epoch loss: 0.1071 - avg acc: 92.4% - val-roc: 0.7602 - val-ap: 0.7605 (0.1s/epoch)
Epoch  843/1000:Epoch loss: 0.1280 - avg acc: 93.1% - val-roc: 0.7959 - val-ap: 0.8192 (0.1s/epoch)
Epoch  844/1000:Epoch loss: 0.1116 - avg acc: 92.4% - val-roc: 0.8112 - val-ap: 0.8537 (0.1s/epoch)
Epoch  845/1000:Epoch loss: 0.1650 - avg acc: 94.7% - val-roc: 0.8214 - val-ap: 0.8427 (0.1s/epoch)
Epoch  846/1000:Epoch loss: 0.2058 - avg acc: 90.1% - val-roc: 0.8061 - val-ap: 0.8143 (0.1s/epoch)
Epoch  847/1000:Epoch loss: 0.1478 - avg acc: 90.8% - val-roc: 0.8112 - val-ap: 0.8206 (0.1s/epoch)
Epoch  848/1000:Epoch loss: 0.1509 - avg acc: 94.7% - val-roc: 0.7806 - val-ap: 0.7598 (0.1s/epoch)
Epoch  849/1000:Epoch loss: 0.1680 - avg acc: 91.6% - val-roc: 0.7755 - val-ap: 0.7535 (0.1s/epoch)
Epoch  850/1000:Epoch loss: 0.1665 - avg acc: 93.9% - val-roc: 0.7755 - val-ap: 0.7135 (0.1s/epoch)
Epoch  851/1000:Epoch loss: 0.1385 - avg acc: 92.4% - val-roc: 0.7245 - val-ap: 0.6766 (0.1s/epoch)
Epoch  852/1000:Epoch loss: 0.1504 - avg acc: 91.6% - val-roc: 0.7398 - val-ap: 0.7240 (0.1s/epoch)
Epoch  853/1000:Epoch loss: 0.1655 - avg acc: 95.4% - val-roc: 0.7602 - val-ap: 0.7136 (0.1s/epoch)
Epoch  854/1000:Epoch loss: 0.1800 - avg acc: 91.6% - val-roc: 0.7908 - val-ap: 0.7627 (0.1s/epoch)
Epoch  855/1000:Epoch loss: 0.1617 - avg acc: 94.7% - val-roc: 0.7857 - val-ap: 0.7610 (0.1s/epoch)
Epoch  856/1000:Epoch loss: 0.1467 - avg acc: 90.8% - val-roc: 0.7653 - val-ap: 0.7687 (0.1s/epoch)
Epoch  857/1000:Epoch loss: 0.1712 - avg acc: 94.7% - val-roc: 0.7704 - val-ap: 0.7176 (0.1s/epoch)
Epoch  858/1000:Epoch loss: 0.1453 - avg acc: 91.6% - val-roc: 0.7092 - val-ap: 0.6663 (0.1s/epoch)
Epoch  859/1000:Epoch loss: 0.1872 - avg acc: 91.6% - val-roc: 0.7143 - val-ap: 0.6682 (0.1s/epoch)
Epoch  860/1000:Epoch loss: 0.1446 - avg acc: 93.9% - val-roc: 0.7245 - val-ap: 0.6734 (0.1s/epoch)
Epoch  861/1000:Epoch loss: 0.1621 - avg acc: 90.8% - val-roc: 0.7704 - val-ap: 0.7680 (0.1s/epoch)
Epoch  862/1000:Epoch loss: 0.1534 - avg acc: 92.4% - val-roc: 0.8418 - val-ap: 0.8795 (0.1s/epoch)
Epoch  863/1000:Epoch loss: 0.1178 - avg acc: 94.7% - val-roc: 0.8265 - val-ap: 0.8568 (0.1s/epoch)
Epoch  864/1000:Epoch loss: 0.1275 - avg acc: 91.6% - val-roc: 0.7857 - val-ap: 0.8114 (0.1s/epoch)
Epoch  865/1000:Epoch loss: 0.2101 - avg acc: 93.1% - val-roc: 0.7806 - val-ap: 0.7964 (0.1s/epoch)
Epoch  866/1000:Epoch loss: 0.1162 - avg acc: 93.9% - val-roc: 0.7755 - val-ap: 0.7719 (0.1s/epoch)
Epoch  867/1000:Epoch loss: 0.1193 - avg acc: 93.1% - val-roc: 0.7755 - val-ap: 0.7719 (0.1s/epoch)
Epoch  868/1000:Epoch loss: 0.1168 - avg acc: 93.1% - val-roc: 0.7755 - val-ap: 0.7723 (0.1s/epoch)
Epoch  869/1000:Epoch loss: 0.0967 - avg acc: 93.9% - val-roc: 0.7959 - val-ap: 0.7954 (0.2s/epoch)
Epoch  870/1000:Epoch loss: 0.1007 - avg acc: 94.7% - val-roc: 0.8112 - val-ap: 0.8186 (0.2s/epoch)
Epoch  871/1000:Epoch loss: 0.1221 - avg acc: 93.1% - val-roc: 0.7551 - val-ap: 0.7619 (0.1s/epoch)
Epoch  872/1000:Epoch loss: 0.1187 - avg acc: 93.1% - val-roc: 0.7500 - val-ap: 0.7594 (0.1s/epoch)
Epoch  873/1000:Epoch loss: 0.1426 - avg acc: 93.1% - val-roc: 0.7449 - val-ap: 0.7571 (0.1s/epoch)
Epoch  874/1000:Epoch loss: 0.1250 - avg acc: 93.1% - val-roc: 0.7398 - val-ap: 0.7554 (0.1s/epoch)
Epoch  875/1000:Epoch loss: 0.1868 - avg acc: 96.2% - val-roc: 0.7653 - val-ap: 0.7719 (0.1s/epoch)
Epoch  876/1000:Epoch loss: 0.1339 - avg acc: 90.8% - val-roc: 0.7857 - val-ap: 0.7823 (0.1s/epoch)
Epoch  877/1000:Epoch loss: 0.1797 - avg acc: 93.1% - val-roc: 0.7806 - val-ap: 0.7547 (0.1s/epoch)
Epoch  878/1000:Epoch loss: 0.1310 - avg acc: 90.8% - val-roc: 0.7755 - val-ap: 0.7766 (0.1s/epoch)
Epoch  879/1000:Epoch loss: 0.1050 - avg acc: 95.4% - val-roc: 0.8010 - val-ap: 0.8004 (0.1s/epoch)
Epoch  880/1000:Epoch loss: 0.1999 - avg acc: 92.4% - val-roc: 0.7704 - val-ap: 0.7752 (0.1s/epoch)
Epoch  881/1000:Epoch loss: 0.1187 - avg acc: 93.9% - val-roc: 0.7347 - val-ap: 0.7289 (0.1s/epoch)
Epoch  882/1000:Epoch loss: 0.1355 - avg acc: 94.7% - val-roc: 0.7143 - val-ap: 0.6747 (0.1s/epoch)
Epoch  883/1000:Epoch loss: 0.1463 - avg acc: 90.8% - val-roc: 0.8061 - val-ap: 0.8017 (0.1s/epoch)
Epoch  884/1000:Epoch loss: 0.1776 - avg acc: 93.1% - val-roc: 0.8776 - val-ap: 0.9077 (0.1s/epoch)
Epoch  885/1000:Epoch loss: 0.1875 - avg acc: 93.1% - val-roc: 0.8520 - val-ap: 0.8921 (0.1s/epoch)
Epoch  886/1000:Epoch loss: 0.1688 - avg acc: 96.9% - val-roc: 0.7653 - val-ap: 0.7896 (0.1s/epoch)
Epoch  887/1000:Epoch loss: 0.1708 - avg acc: 95.4% - val-roc: 0.7602 - val-ap: 0.7492 (0.1s/epoch)
Epoch  888/1000:Epoch loss: 0.1073 - avg acc: 93.9% - val-roc: 0.8214 - val-ap: 0.8328 (0.1s/epoch)
Epoch  889/1000:Epoch loss: 0.1658 - avg acc: 93.1% - val-roc: 0.8214 - val-ap: 0.8321 (0.1s/epoch)
Epoch  890/1000:Epoch loss: 0.1716 - avg acc: 90.1% - val-roc: 0.7908 - val-ap: 0.7931 (0.1s/epoch)
Epoch  891/1000:Epoch loss: 0.1397 - avg acc: 94.7% - val-roc: 0.7857 - val-ap: 0.7693 (0.1s/epoch)
Epoch  892/1000:Epoch loss: 0.1127 - avg acc: 90.8% - val-roc: 0.7857 - val-ap: 0.7597 (0.1s/epoch)
Epoch  893/1000:Epoch loss: 0.1390 - avg acc: 94.7% - val-roc: 0.7806 - val-ap: 0.7142 (0.1s/epoch)
Epoch  894/1000:Epoch loss: 0.1423 - avg acc: 92.4% - val-roc: 0.7551 - val-ap: 0.6952 (0.1s/epoch)
Epoch  895/1000:Epoch loss: 0.1192 - avg acc: 95.4% - val-roc: 0.8010 - val-ap: 0.7334 (0.1s/epoch)
Epoch  896/1000:Epoch loss: 0.1139 - avg acc: 90.1% - val-roc: 0.8061 - val-ap: 0.7427 (0.1s/epoch)
Epoch  897/1000:Epoch loss: 0.1075 - avg acc: 90.8% - val-roc: 0.7908 - val-ap: 0.7234 (0.1s/epoch)
Epoch  898/1000:Epoch loss: 0.1909 - avg acc: 93.9% - val-roc: 0.7755 - val-ap: 0.7060 (0.1s/epoch)
Epoch  899/1000:Epoch loss: 0.1526 - avg acc: 80.9% - val-roc: 0.7398 - val-ap: 0.6816 (0.1s/epoch)
Epoch  900/1000:Epoch loss: 0.1509 - avg acc: 93.9% - val-roc: 0.7755 - val-ap: 0.7235 (0.1s/epoch)
Epoch  901/1000:Epoch loss: 0.1086 - avg acc: 92.4% - val-roc: 0.7755 - val-ap: 0.7600 (0.1s/epoch)
Epoch  902/1000:Epoch loss: 0.1674 - avg acc: 92.4% - val-roc: 0.8163 - val-ap: 0.8123 (0.1s/epoch)
Epoch  903/1000:Epoch loss: 0.1070 - avg acc: 91.6% - val-roc: 0.8418 - val-ap: 0.8561 (0.1s/epoch)
Epoch  904/1000:Epoch loss: 0.1487 - avg acc: 93.1% - val-roc: 0.8469 - val-ap: 0.8583 (0.1s/epoch)
Epoch  905/1000:Epoch loss: 0.0904 - avg acc: 92.4% - val-roc: 0.7755 - val-ap: 0.7850 (0.1s/epoch)
Epoch  906/1000:Epoch loss: 0.0907 - avg acc: 92.4% - val-roc: 0.7398 - val-ap: 0.7527 (0.1s/epoch)
Epoch  907/1000:Epoch loss: 0.1323 - avg acc: 93.9% - val-roc: 0.7602 - val-ap: 0.7671 (0.1s/epoch)
Epoch  908/1000:Epoch loss: 0.1115 - avg acc: 94.7% - val-roc: 0.8214 - val-ap: 0.7938 (0.1s/epoch)
Epoch  909/1000:Epoch loss: 0.1482 - avg acc: 93.9% - val-roc: 0.7806 - val-ap: 0.7657 (0.1s/epoch)
Epoch  910/1000:Epoch loss: 0.1176 - avg acc: 94.7% - val-roc: 0.7449 - val-ap: 0.6991 (0.1s/epoch)
Epoch  911/1000:Epoch loss: 0.1392 - avg acc: 96.2% - val-roc: 0.7347 - val-ap: 0.6745 (0.1s/epoch)
Epoch  912/1000:Epoch loss: 0.1152 - avg acc: 94.7% - val-roc: 0.7347 - val-ap: 0.6959 (0.1s/epoch)
Epoch  913/1000:Epoch loss: 0.1415 - avg acc: 93.9% - val-roc: 0.7857 - val-ap: 0.7705 (0.1s/epoch)
Epoch  914/1000:Epoch loss: 0.1238 - avg acc: 93.1% - val-roc: 0.7959 - val-ap: 0.7784 (0.1s/epoch)
Epoch  915/1000:Epoch loss: 0.1350 - avg acc: 92.4% - val-roc: 0.7449 - val-ap: 0.7066 (0.1s/epoch)
Epoch  916/1000:Epoch loss: 0.1459 - avg acc: 93.9% - val-roc: 0.7602 - val-ap: 0.7234 (0.1s/epoch)
Epoch  917/1000:Epoch loss: 0.1258 - avg acc: 92.4% - val-roc: 0.6990 - val-ap: 0.6748 (0.1s/epoch)
Epoch  918/1000:Epoch loss: 0.1842 - avg acc: 95.4% - val-roc: 0.7959 - val-ap: 0.7824 (0.1s/epoch)
Epoch  919/1000:Epoch loss: 0.1404 - avg acc: 93.1% - val-roc: 0.7959 - val-ap: 0.7810 (0.1s/epoch)
Epoch  920/1000:Epoch loss: 0.1099 - avg acc: 93.1% - val-roc: 0.7704 - val-ap: 0.7533 (0.1s/epoch)
Epoch  921/1000:Epoch loss: 0.1021 - avg acc: 94.7% - val-roc: 0.8010 - val-ap: 0.7798 (0.1s/epoch)
Epoch  922/1000:Epoch loss: 0.1406 - avg acc: 93.1% - val-roc: 0.8214 - val-ap: 0.8120 (0.1s/epoch)
Epoch  923/1000:Epoch loss: 0.0960 - avg acc: 93.9% - val-roc: 0.8214 - val-ap: 0.8277 (0.1s/epoch)
Epoch  924/1000:Epoch loss: 0.1476 - avg acc: 93.1% - val-roc: 0.8316 - val-ap: 0.8212 (0.1s/epoch)
Epoch  925/1000:Epoch loss: 0.1504 - avg acc: 91.6% - val-roc: 0.8010 - val-ap: 0.8026 (0.1s/epoch)
Epoch  926/1000:Epoch loss: 0.1140 - avg acc: 94.7% - val-roc: 0.7857 - val-ap: 0.7357 (0.1s/epoch)
Epoch  927/1000:Epoch loss: 0.0993 - avg acc: 92.4% - val-roc: 0.7092 - val-ap: 0.6782 (0.1s/epoch)
Epoch  928/1000:Epoch loss: 0.1967 - avg acc: 93.1% - val-roc: 0.7347 - val-ap: 0.7322 (0.1s/epoch)
Epoch  929/1000:Epoch loss: 0.0909 - avg acc: 93.9% - val-roc: 0.7704 - val-ap: 0.7674 (0.1s/epoch)
Epoch  930/1000:Epoch loss: 0.1463 - avg acc: 92.4% - val-roc: 0.7602 - val-ap: 0.7798 (0.1s/epoch)
Epoch  931/1000:Epoch loss: 0.1203 - avg acc: 96.9% - val-roc: 0.7806 - val-ap: 0.7700 (0.1s/epoch)
Epoch  932/1000:Epoch loss: 0.1316 - avg acc: 95.4% - val-roc: 0.7755 - val-ap: 0.7664 (0.1s/epoch)
Epoch  933/1000:Epoch loss: 0.1792 - avg acc: 90.1% - val-roc: 0.7398 - val-ap: 0.7623 (0.1s/epoch)
Epoch  934/1000:Epoch loss: 0.1625 - avg acc: 97.7% - val-roc: 0.7755 - val-ap: 0.7916 (0.1s/epoch)
Epoch  935/1000:Epoch loss: 0.1515 - avg acc: 84.7% - val-roc: 0.7653 - val-ap: 0.7931 (0.1s/epoch)
Epoch  936/1000:Epoch loss: 0.1555 - avg acc: 95.4% - val-roc: 0.7755 - val-ap: 0.7974 (0.1s/epoch)
Epoch  937/1000:Epoch loss: 0.1296 - avg acc: 91.6% - val-roc: 0.7347 - val-ap: 0.7535 (0.1s/epoch)
Epoch  938/1000:Epoch loss: 0.1297 - avg acc: 93.9% - val-roc: 0.7653 - val-ap: 0.7923 (0.1s/epoch)
Epoch  939/1000:Epoch loss: 0.1206 - avg acc: 93.1% - val-roc: 0.7857 - val-ap: 0.8016 (0.1s/epoch)
Epoch  940/1000:Epoch loss: 0.1575 - avg acc: 92.4% - val-roc: 0.7704 - val-ap: 0.7799 (0.1s/epoch)
Epoch  941/1000:Epoch loss: 0.1030 - avg acc: 93.1% - val-roc: 0.7755 - val-ap: 0.7997 (0.1s/epoch)
Epoch  942/1000:Epoch loss: 0.1082 - avg acc: 90.8% - val-roc: 0.7653 - val-ap: 0.7782 (0.1s/epoch)
Epoch  943/1000:Epoch loss: 0.1285 - avg acc: 93.1% - val-roc: 0.7653 - val-ap: 0.7775 (0.1s/epoch)
Epoch  944/1000:Epoch loss: 0.0796 - avg acc: 94.7% - val-roc: 0.7602 - val-ap: 0.7539 (0.1s/epoch)
Epoch  945/1000:Epoch loss: 0.0982 - avg acc: 91.6% - val-roc: 0.7704 - val-ap: 0.7580 (0.1s/epoch)
Epoch  946/1000:Epoch loss: 0.1599 - avg acc: 93.1% - val-roc: 0.7755 - val-ap: 0.7605 (0.1s/epoch)
Epoch  947/1000:Epoch loss: 0.0929 - avg acc: 90.8% - val-roc: 0.7908 - val-ap: 0.7892 (0.1s/epoch)
Epoch  948/1000:Epoch loss: 0.1180 - avg acc: 93.1% - val-roc: 0.8061 - val-ap: 0.8064 (0.1s/epoch)
Epoch  949/1000:Epoch loss: 0.0927 - avg acc: 93.1% - val-roc: 0.7908 - val-ap: 0.7900 (0.1s/epoch)
Epoch  950/1000:Epoch loss: 0.1616 - avg acc: 95.4% - val-roc: 0.8265 - val-ap: 0.8186 (0.1s/epoch)
Epoch  951/1000:Epoch loss: 0.1549 - avg acc: 93.1% - val-roc: 0.8622 - val-ap: 0.8543 (0.1s/epoch)
Epoch  952/1000:Epoch loss: 0.0953 - avg acc: 95.4% - val-roc: 0.8878 - val-ap: 0.8989 (0.1s/epoch)
Epoch  953/1000:Epoch loss: 0.1740 - avg acc: 93.1% - val-roc: 0.8776 - val-ap: 0.8926 (0.1s/epoch)
Epoch  954/1000:Epoch loss: 0.1464 - avg acc: 90.8% - val-roc: 0.8878 - val-ap: 0.9138 (0.1s/epoch)
Epoch  955/1000:Epoch loss: 0.2530 - avg acc: 95.4% - val-roc: 0.8929 - val-ap: 0.9229 (0.1s/epoch)
Epoch  956/1000:Epoch loss: 0.1995 - avg acc: 84.7% - val-roc: 0.8112 - val-ap: 0.8223 (0.1s/epoch)
Epoch  957/1000:Epoch loss: 0.1961 - avg acc: 96.2% - val-roc: 0.8010 - val-ap: 0.8031 (0.1s/epoch)
Epoch  958/1000:Epoch loss: 0.0982 - avg acc: 91.6% - val-roc: 0.7653 - val-ap: 0.7550 (0.1s/epoch)
Epoch  959/1000:Epoch loss: 0.1516 - avg acc: 93.9% - val-roc: 0.7653 - val-ap: 0.7550 (0.1s/epoch)
Epoch  960/1000:Epoch loss: 0.1010 - avg acc: 93.1% - val-roc: 0.7908 - val-ap: 0.7890 (0.1s/epoch)
Epoch  961/1000:Epoch loss: 0.1319 - avg acc: 90.1% - val-roc: 0.8112 - val-ap: 0.8002 (0.1s/epoch)
Epoch  962/1000:Epoch loss: 0.1855 - avg acc: 94.7% - val-roc: 0.8265 - val-ap: 0.7944 (0.1s/epoch)
Epoch  963/1000:Epoch loss: 0.0799 - avg acc: 93.9% - val-roc: 0.7806 - val-ap: 0.7591 (0.1s/epoch)
Epoch  964/1000:Epoch loss: 0.1547 - avg acc: 90.8% - val-roc: 0.8214 - val-ap: 0.8463 (0.1s/epoch)
Epoch  965/1000:Epoch loss: 0.1527 - avg acc: 93.9% - val-roc: 0.8469 - val-ap: 0.8766 (0.1s/epoch)
Epoch  966/1000:Epoch loss: 0.2005 - avg acc: 96.2% - val-roc: 0.8520 - val-ap: 0.8790 (0.1s/epoch)
Epoch  967/1000:Epoch loss: 0.0918 - avg acc: 93.1% - val-roc: 0.7704 - val-ap: 0.7961 (0.1s/epoch)
Epoch  968/1000:Epoch loss: 0.2136 - avg acc: 93.9% - val-roc: 0.7857 - val-ap: 0.8015 (0.1s/epoch)
Epoch  969/1000:Epoch loss: 0.1549 - avg acc: 93.9% - val-roc: 0.8214 - val-ap: 0.8464 (0.1s/epoch)
Epoch  970/1000:Epoch loss: 0.1310 - avg acc: 88.5% - val-roc: 0.8214 - val-ap: 0.8545 (0.1s/epoch)
Epoch  971/1000:Epoch loss: 0.1553 - avg acc: 93.1% - val-roc: 0.8367 - val-ap: 0.8618 (0.1s/epoch)
Epoch  972/1000:Epoch loss: 0.1305 - avg acc: 93.1% - val-roc: 0.8061 - val-ap: 0.8219 (0.1s/epoch)
Epoch  973/1000:Epoch loss: 0.1567 - avg acc: 93.1% - val-roc: 0.8214 - val-ap: 0.8171 (0.1s/epoch)
Epoch  974/1000:Epoch loss: 0.1165 - avg acc: 93.9% - val-roc: 0.8571 - val-ap: 0.8611 (0.1s/epoch)
Epoch  975/1000:Epoch loss: 0.1220 - avg acc: 93.1% - val-roc: 0.8571 - val-ap: 0.8700 (0.1s/epoch)
Epoch  976/1000:Epoch loss: 0.1077 - avg acc: 94.7% - val-roc: 0.8316 - val-ap: 0.8267 (0.1s/epoch)
Epoch  977/1000:Epoch loss: 0.1349 - avg acc: 94.7% - val-roc: 0.8316 - val-ap: 0.8271 (0.1s/epoch)
Epoch  978/1000:Epoch loss: 0.1499 - avg acc: 94.7% - val-roc: 0.8571 - val-ap: 0.8589 (0.1s/epoch)
Epoch  979/1000:Epoch loss: 0.0999 - avg acc: 93.1% - val-roc: 0.8673 - val-ap: 0.8850 (0.1s/epoch)
Epoch  980/1000:Epoch loss: 0.1020 - avg acc: 96.9% - val-roc: 0.8827 - val-ap: 0.8929 (0.1s/epoch)
Epoch  981/1000:Epoch loss: 0.1597 - avg acc: 94.7% - val-roc: 0.8418 - val-ap: 0.8564 (0.1s/epoch)
Epoch  982/1000:Epoch loss: 0.1340 - avg acc: 93.9% - val-roc: 0.8265 - val-ap: 0.8393 (0.1s/epoch)
Epoch  983/1000:Epoch loss: 0.1146 - avg acc: 93.9% - val-roc: 0.8316 - val-ap: 0.8454 (0.1s/epoch)
Epoch  984/1000:Epoch loss: 0.1013 - avg acc: 92.4% - val-roc: 0.7908 - val-ap: 0.8149 (0.1s/epoch)
Epoch  985/1000:Epoch loss: 0.1193 - avg acc: 95.4% - val-roc: 0.8112 - val-ap: 0.8358 (0.1s/epoch)
Epoch  986/1000:Epoch loss: 0.1677 - avg acc: 92.4% - val-roc: 0.7806 - val-ap: 0.7871 (0.1s/epoch)
Epoch  987/1000:Epoch loss: 0.1145 - avg acc: 95.4% - val-roc: 0.8265 - val-ap: 0.8182 (0.1s/epoch)
Epoch  988/1000:Epoch loss: 0.1158 - avg acc: 92.4% - val-roc: 0.8418 - val-ap: 0.8261 (0.1s/epoch)
Epoch  989/1000:Epoch loss: 0.1163 - avg acc: 94.7% - val-roc: 0.8622 - val-ap: 0.8375 (0.1s/epoch)
Epoch  990/1000:Epoch loss: 0.0995 - avg acc: 93.1% - val-roc: 0.8265 - val-ap: 0.8152 (0.1s/epoch)
Epoch  991/1000:Epoch loss: 0.1288 - avg acc: 93.1% - val-roc: 0.8010 - val-ap: 0.7669 (0.1s/epoch)
Epoch  992/1000:Epoch loss: 0.1355 - avg acc: 95.4% - val-roc: 0.7908 - val-ap: 0.7636 (0.1s/epoch)
Epoch  993/1000:Epoch loss: 0.1664 - avg acc: 92.4% - val-roc: 0.7755 - val-ap: 0.7782 (0.1s/epoch)
Epoch  994/1000:Epoch loss: 0.1512 - avg acc: 93.1% - val-roc: 0.8418 - val-ap: 0.8651 (0.1s/epoch)
Epoch  995/1000:Epoch loss: 0.1034 - avg acc: 95.4% - val-roc: 0.8622 - val-ap: 0.8814 (0.1s/epoch)
Epoch  996/1000:Epoch loss: 0.1269 - avg acc: 91.6% - val-roc: 0.8316 - val-ap: 0.8516 (0.1s/epoch)
Epoch  997/1000:Epoch loss: 0.1327 - avg acc: 93.9% - val-roc: 0.8214 - val-ap: 0.8459 (0.1s/epoch)
Epoch  998/1000:Epoch loss: 0.1132 - avg acc: 95.4% - val-roc: 0.8316 - val-ap: 0.8596 (0.1s/epoch)
Epoch  999/1000:Epoch loss: 0.1247 - avg acc: 93.1% - val-roc: 0.8622 - val-ap: 0.8813 (0.1s/epoch)
Epoch  1000/1000:Epoch loss: 0.1914 - avg acc: 92.4% - val-roc: 0.8673 - val-ap: 0.8881 (0.1s/epoch)
Done!
Test ROC: 0.8667 - Test AP: 0.9531
</pre></div>
</div>
</div>
</div>
</section>
<section id="plots-of-training-history">
<h3>Plots of training history<a class="headerlink" href="#plots-of-training-history" title="Permalink to this heading">#</a></h3>
<p>Let’s see how the training went :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">utils</span><span class="o">.</span><span class="n">plot_history</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d3d7dd5cd92c28c89ce282dc72321952d0f6f7391aae26697e979ac2aa53f534.png" src="../_images/d3d7dd5cd92c28c89ce282dc72321952d0f6f7391aae26697e979ac2aa53f534.png" />
</div>
</div>
</section>
</section>
<section id="validation-on-unseen-data">
<h2>Validation on unseen data<a class="headerlink" href="#validation-on-unseen-data" title="Permalink to this heading">#</a></h2>
<p>Now, let’s use our held-out data to check how well our model handles unseen data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_roc</span><span class="p">,</span> <span class="n">test_ap</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">test</span><span class="p">(</span>
    <span class="n">dataloaders</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">return_preds</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test ROC-AUC: </span><span class="si">{</span><span class="n">test_roc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test AP: </span><span class="si">{</span><span class="n">test_ap</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plot the ROC curve</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Number of correct positive predictions on test set: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">preds</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="p">(</span><span class="n">ys</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2"> out of </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ys</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Number of correct negative predictions on test set: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">preds</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="p">(</span><span class="n">ys</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">))</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2"> out of </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ys</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test ROC-AUC: 0.8667
Test AP: 0.9531
</pre></div>
</div>
<img alt="../_images/433d87361ad7c489b88725e7eeb60ff7a0c7e2db608739b540c93d237f1c03a0.png" src="../_images/433d87361ad7c489b88725e7eeb60ff7a0c7e2db608739b540c93d237f1c03a0.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of correct positive predictions on test set: 17 out of 20
Number of correct negative predictions on test set: 8 out of 9
</pre></div>
</div>
</div>
</div>
<p>We can also look at the distribution of predictions, to get a sense of our model’s uncertainty :</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">utils</span><span class="o">.</span><span class="n">show_preds_distribution</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c2309dd8f76271abaab6be8bbde0b2ea7813e91af3824132b2d7f14e8b10c16a.png" src="../_images/c2309dd8f76271abaab6be8bbde0b2ea7813e91af3824132b2d7f14e8b10c16a.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>This plot shows the distribution of the labels and predictions;
predictions are overlayed on top of the labels,
showing whether they are missing or surnumerous.
The labels are 20 positive and 9 negative.
The predictions are 18 positive and 11 negative.
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">utils</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2bc006b75ca0d09702b5eb62f6659b09a9c0edf357d50080db65fd78d78094a9.png" src="../_images/2bc006b75ca0d09702b5eb62f6659b09a9c0edf357d50080db65fd78d78094a9.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mislabeled</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">find_mislabeled_molecules</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">mislabeled</span><span class="p">)</span><span class="si">}</span><span class="s2"> mislabeled molecules&quot;</span><span class="p">)</span>
<span class="n">mols</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">id_mol</span> <span class="ow">in</span> <span class="n">mislabeled</span><span class="p">:</span>
    <span class="n">mols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="p">[</span><span class="n">id_mol</span><span class="p">])</span>

<span class="n">thresh_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">preds</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)[</span><span class="n">mislabeled</span><span class="p">]</span>
<span class="n">utils</span><span class="o">.</span><span class="n">draw_molecule_from_dict</span><span class="p">(</span>
    <span class="n">mols</span><span class="p">[:</span><span class="mi">5</span><span class="p">],</span>
    <span class="n">preds</span><span class="o">=</span><span class="n">thresh_preds</span><span class="p">,</span>
    <span class="n">mol_ids</span><span class="o">=</span><span class="n">mislabeled</span><span class="p">,</span>
    <span class="n">n_cols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">n_rows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>4 mislabeled molecules
</pre></div>
</div>
<img alt="../_images/00c203ad0d12d7cb5d5efad831c822ef429fac4ac0d55fc607bf1bfd566eefa3.png" src="../_images/00c203ad0d12d7cb5d5efad831c822ef429fac4ac0d55fc607bf1bfd566eefa3.png" />
</div>
</div>
</section>
<section id="full-dataset-performance">
<h2>Full dataset performance<a class="headerlink" href="#full-dataset-performance" title="Permalink to this heading">#</a></h2>
<p>Out of curiosity, let’s see how the model performs on the whole dataset.</p>
<p>We will as before plot the ROC curve and the distribution of predictions;
additionally, we will show which molecules the model misclassified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data_all</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">create_dataset_dict</span><span class="p">(</span><span class="n">add_edge_features</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">full_dataset</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">MutagDataset</span><span class="p">(</span><span class="n">data_all</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="full-roc-curve">
<h3>Full ROC curve<a class="headerlink" href="#full-roc-curve" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_roc</span><span class="p">,</span> <span class="n">full_ap</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">ys</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">test</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">return_preds</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Full ROC-AUC: </span><span class="si">{</span><span class="n">full_roc</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Full AP: </span><span class="si">{</span><span class="n">full_ap</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plot the ROC curve</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">ys</span> <span class="o">=</span> <span class="n">ys</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Number of correct positive predictions on test set: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">preds</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mf">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="p">(</span><span class="n">ys</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">))</span><span class="si">}</span><span class="s2"> out of </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ys</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span>
    <span class="sa">f</span><span class="s2">&quot;Number of correct negative predictions on test set: </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">preds</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="w"> </span><span class="p">(</span><span class="n">ys</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">))</span><span class="si">}</span><span class="s2"> out of </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ys</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Full ROC-AUC: 0.9545
Full AP: 0.9784
</pre></div>
</div>
<img alt="../_images/7a4a8a800e487be0af8d5bc09b42d987c7ce2da1c65c157dae458a1ec8de96ae.png" src="../_images/7a4a8a800e487be0af8d5bc09b42d987c7ce2da1c65c157dae458a1ec8de96ae.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of correct positive predictions on test set: 119 out of 125
Number of correct negative predictions on test set: 48 out of 63
</pre></div>
</div>
</div>
</div>
</section>
<section id="predictions-distribution">
<h3>Predictions distribution<a class="headerlink" href="#predictions-distribution" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">utils</span><span class="o">.</span><span class="n">show_preds_distribution</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/3ce0642972d7a6049b5823885c7f629ce9adb4b19cf4e20922983a82afcd0ad5.png" src="../_images/3ce0642972d7a6049b5823885c7f629ce9adb4b19cf4e20922983a82afcd0ad5.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>This plot shows the distribution of the labels and predictions;
predictions are overlayed on top of the labels,
showing whether they are missing or surnumerous.
The labels are 125 positive and 63 negative.
The predictions are 134 positive and 54 negative.
</pre></div>
</div>
</div>
</div>
</section>
<section id="confusion-matrix">
<h3>Confusion matrix<a class="headerlink" href="#confusion-matrix" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">utils</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f47e024130fbfdd538dbc1c604dc6a4f2bbf22e3c0210d07468feb5d5bf5ade8.png" src="../_images/f47e024130fbfdd538dbc1c604dc6a4f2bbf22e3c0210d07468feb5d5bf5ade8.png" />
</div>
</div>
</section>
<section id="check-mislabelled-molecules">
<h3>Check mislabelled molecules<a class="headerlink" href="#check-mislabelled-molecules" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mislabeled</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">find_mislabeled_molecules</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">ys</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">mislabeled</span><span class="p">)</span><span class="si">}</span><span class="s2"> mislabeled molecules&quot;</span><span class="p">)</span>
<span class="n">mols</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">id_mol</span> <span class="ow">in</span> <span class="n">mislabeled</span><span class="p">:</span>
    <span class="n">mols</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">full_dataset</span><span class="p">[</span><span class="n">id_mol</span><span class="p">])</span>

<span class="n">thresh_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">preds</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)[</span><span class="n">mislabeled</span><span class="p">]</span>
<span class="n">utils</span><span class="o">.</span><span class="n">draw_molecule_from_dict</span><span class="p">(</span>
    <span class="n">mols</span><span class="p">[:</span><span class="mi">20</span><span class="p">],</span>
    <span class="n">preds</span><span class="o">=</span><span class="n">thresh_preds</span><span class="p">,</span>
    <span class="n">mol_ids</span><span class="o">=</span><span class="n">mislabeled</span><span class="p">,</span>
    <span class="n">n_cols</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">n_rows</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>21 mislabeled molecules
</pre></div>
</div>
<img alt="../_images/0a5da6c81a7461096cb97158058f146d3013f9acc55b3ca571474c5444b098b7.png" src="../_images/0a5da6c81a7461096cb97158058f146d3013f9acc55b3ca571474c5444b098b7.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./rendered_notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="../wandb_comparisons/best_runs.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Summary of model performance</p>
      </div>
    </a>
    <a class="right-next"
       href="SAGE.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">GraphSAGE</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#description">Description</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-loading">Dataset loading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameters">Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-loop">Training loop</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#plots-of-training-history">Plots of training history</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-on-unseen-data">Validation on unseen data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#full-dataset-performance">Full dataset performance</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#full-roc-curve">Full ROC curve</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions-distribution">Predictions distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion matrix</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-mislabelled-molecules">Check mislabelled molecules</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Cyril Achard
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=c5ced968eda925caa686"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=c5ced968eda925caa686"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>