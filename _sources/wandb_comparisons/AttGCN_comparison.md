# Hyperparameter tuning for Attention GCN

## Global hyperparameters

Below is a comparison when tuning hyperparameters for the Attention GCN model.
Those parameters included:

* Learning rate
* Using the adjacency matrix to multiply with the attention scores or not
* Using mean or max pooling
* Adding an extra layer of dimension 32 at the end of the model

<iframe src="https://wandb.ai/c-achard/DL%20Biomed%20Homework%202%20-%20v3/reports/Attention-GCN-tuning--Vmlldzo1NzM2NTI3" style="border:none;height:1024px;width:100%"> </iframe>

## Links to reports

1) [Attention GCN Report](https://api.wandb.ai/links/c-achard/24ws9ecw)
